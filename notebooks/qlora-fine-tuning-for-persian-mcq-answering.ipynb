{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14146021,"sourceType":"datasetVersion","datasetId":9015410},{"sourceId":14146636,"sourceType":"datasetVersion","datasetId":9015894}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from openai import OpenAI\nfrom kaggle_secrets import UserSecretsClient\nimport re\nimport json\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:22:50.326086Z","iopub.execute_input":"2025-12-14T15:22:50.326370Z","iopub.status.idle":"2025-12-14T15:22:50.824882Z","shell.execute_reply.started":"2025-12-14T15:22:50.326345Z","shell.execute_reply":"2025-12-14T15:22:50.824061Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"api_key\")\nBased_url = \"https://api.avalai.ir/v1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:22:50.878381Z","iopub.execute_input":"2025-12-14T15:22:50.879041Z","iopub.status.idle":"2025-12-14T15:22:51.003716Z","shell.execute_reply.started":"2025-12-14T15:22:50.879019Z","shell.execute_reply":"2025-12-14T15:22:51.003201Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 1) Load the source and create chunks","metadata":{}},{"cell_type":"code","source":"def load_chunks(path: str):\n    text = open(path, \"r\", encoding=\"utf-8\").read()\n    pattern = r\"\\nCHUNK\\s+(\\d{2})\\n\"\n    parts = re.split(pattern, \"\\n\" + text)\n\n    chunks = []\n    for i in range(1, len(parts), 2):\n        num = parts[i]\n        chunk_text = parts[i+1].strip()\n        if chunk_text:\n            chunks.append({\n                \"chunk_id\": f\"CHUNK_{num}\",\n                \"text\": chunk_text\n            })\n    return chunks\n\nchunks = load_chunks(\"/kaggle/input/document/doc.txt\")\nlen(chunks), chunks[0][\"chunk_id\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:22:52.149328Z","iopub.execute_input":"2025-12-14T15:22:52.149616Z","iopub.status.idle":"2025-12-14T15:22:52.163236Z","shell.execute_reply.started":"2025-12-14T15:22:52.149593Z","shell.execute_reply":"2025-12-14T15:22:52.162598Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(16, 'CHUNK_01')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"chunks[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:22:53.756969Z","iopub.execute_input":"2025-12-14T15:22:53.757522Z","iopub.status.idle":"2025-12-14T15:22:53.762287Z","shell.execute_reply.started":"2025-12-14T15:22:53.757494Z","shell.execute_reply":"2025-12-14T15:22:53.761657Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'chunk_id': 'CHUNK_01',\n 'text': 'Transformers “The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759 In this chapter we introduce the transformer, the standard architecture for building large language models. As we discussed in the prior chapter, transformer-based large language models have completely changed the ﬁeld of speech and language processing. Indeed, every subsequent chapter in this textbook will make use of them. As with the previous chapter, we’ll focus for this chapter on the use of transformers to model left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer is a neural network with a speciﬁc structure that includes a mechanism called self-attention or multi-head attention. Attention can be thought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. A transformer has three major components. At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer\\n\\nnormalization steps) that maps an input vector xi in column i (corresponding to input token i) to an output vector hi. The set of n blocks maps an entire context window of input vectors (x1,...,xn) to a window of output vectors (h1,...,hn) of the same length. A column might contain from 12 to 96 or more stacked blocks. The column of blocks is preceded by the input encoding component, which processes an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding output by the ﬁnal transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column. Transformer-based language models are complex, and so the details will unfold over the next few chapters. Chapter 7 already discussed how language models are pretrained, and how tokens are generated via sampling. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components of the transformer. Chapter 10 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 9 shows how to instruction-tune language models to perform NLP tasks, and how to align the model with human preferences. Chapter 12 will introduce machine translation with the encoder-decoder architecture. We’ll see further use of the encoder-decoder architecture in Chapter 15. 8. Attention Recall from Chapter 5 that for word2vec and other static embeddings, the representation of a word’s meaning is always the same vector irrespective of the context: the word chicken, for example, is always represented by the same ﬁxed vector. So a static vector for the word it might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning. Consider it in one of these two sentences: (8.1) The chicken didn’t cross the road because it was too tired. (8.2) The chicken didn’t cross the road because it was too wide. In (8.1) it is the chicken (i.e., the reader knows that the chicken was tired), while in (8.2) it is the road (and the reader knows that the road was wide). That is, if we are to compute the meaning of this sentence, we’ll need the meaning of it to be associated with the chicken in the ﬁrst sentence and associated with the road in the second one, sensitive to the context. Furthermore, consider reading left to right like a causal language model, processing the sentence up to the word it: (8.3) The chicken didn’t cross the road because it At this point we don’t yet know which thing it is going to end up referring to! So a representation of it at this point might have aspects of both chicken and road as the reader is trying to guess what happens next. This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples: (8.4) The keys to the cabinet are on the table.\\n\\n--------------------------------------------------------------------------------'}"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## 2) Generation prompt","metadata":{}},{"cell_type":"code","source":"system_prompt = \"\"\"\nYou are a superintelligent, world-class exam writer and assessment designer.\nTask: Given an English source chunk, generate high-quality Persian (Farsi) multiple-choice questions (4 options) that are strictly grounded in the provided chunk.\nHard constraints:\n- Output MUST be valid JSON only. No markdown, no commentary, no extra text.\n- Questions and options MUST be in Persian.\n- Use ONLY the information in the provided chunk. Do not use external knowledge.\n- Do NOT produce any question that contains mathematical formulas, equations, or math-like notation\n  (including symbols like =, +, -, /, √, α, subscripts, superscripts, matrix notation, or LaTeX).\n  If a concept is mathematical, ask it conceptually in plain Persian words without any formula or symbolic expression.\n- Do NOT copy long exact phrases from the chunk (avoid > 8 consecutive English words). Paraphrase.\n- Each question must have exactly 4 options, provided as a JSON array in order [0,1,2,3].\n- The answer field must be an integer: 0 or 1 or 2 or 3.\n- Avoid duplicates across questions.\n-Avoid Writing questions \nWrite questions as if the student has already studied the topic, not as if they are reading a passage.\nNever mention “chapter/section/text/this chapter”.\n\n\nJSON schema to output:\n{\n  \"chunk_id\": \"<string>\",\n  \"questions\": [\n    {\n      \"id\": \"Q<NUMBER>\",\n      \"question\": \"<Persian question text>\",\n      \"options\": [\"<0>\", \"<1>\", \"<2>\", \"<3>\"],\n      \"answer\": 0\n    }\n  ]\n}\n\"\"\".strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:22:56.558043Z","iopub.execute_input":"2025-12-14T15:22:56.558774Z","iopub.status.idle":"2025-12-14T15:22:56.563059Z","shell.execute_reply.started":"2025-12-14T15:22:56.558719Z","shell.execute_reply":"2025-12-14T15:22:56.562214Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 2.1) Generate questions from chunks and save to `questions.json`","metadata":{}},{"cell_type":"code","source":"client = OpenAI(api_key=API_KEY, base_url=Based_url)\nopenai_model = \"gpt-5-mini\"\n\nnum_questions = 10\nstart_number = 1\n\nall_questions = []\nby_chunk = []\n\nfor ch in chunks[0:11]:\n    chunk_id = ch[\"chunk_id\"]\n    chunk_text = ch[\"text\"]\n\n    user_prompt = f\"\"\"\nchunk_id: {chunk_id}\nstart_number: {start_number}\nnum_questions: {num_questions}\n\nSOURCE CHUNK (English):\n\\\"\\\"\\\"\n{chunk_text}\n\\\"\\\"\\\"\n\nGenerate exactly num_questions questions.\nIDs must be sequential starting from start_number (Q001, Q002, ...).\nOutput JSON only.\nImportant: Shuffle options per question and vary answer indices across the batch.\n\"\"\".strip()\n\n    resp = client.chat.completions.create(\n        model=openai_model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        temperature=0.4\n    )\n\n    raw = resp.choices[0].message.content\n    obj = json.loads(raw)\n\n    by_chunk.append(obj)\n    all_questions.extend(obj[\"questions\"])\n\n    start_number += num_questions\n\nfinal_obj = {\n    \"total_questions\": len(all_questions),\n    \"chunks_used\": len(by_chunk),\n    \"questions\": all_questions,\n    \"by_chunk\": by_chunk\n}\n\nwith open(\"questions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(final_obj, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved: questions.json | total:\", len(all_questions))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/questions/questions.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nfor q in data[\"questions\"][:5]:\n    print(q[\"id\"], q[\"question\"])\n    print(\"options:\", q[\"options\"])\n    print(\"answer:\", q[\"answer\"])\n    print(\"-\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:23:00.558443Z","iopub.execute_input":"2025-12-14T15:23:00.559187Z","iopub.status.idle":"2025-12-14T15:23:00.567455Z","shell.execute_reply.started":"2025-12-14T15:23:00.559155Z","shell.execute_reply":"2025-12-14T15:23:00.566787Z"}},"outputs":[{"name":"stdout","text":"Q001 مکانیسمی که در ترنسفورمر برای ساختن نمایش‌های متنیِ متن-محور یک توکن استفاده می‌شود کدام است؟\noptions: ['توجهِ خودی یا چندسَرِ توجه', 'پردازش کانولوشنی', 'نمونه\\u200cبرداری تصادفی', 'شبکهٔ بازگشتی کلاسیک']\nanswer: 0\n----------------------------------------\nQ002 در مدل‌سازی زبانیِ چپ‌به‌راست (خودِ علی)، پیش‌بینی توکن‌ها به چه صورت انجام می‌شود؟\noptions: ['همهٔ توکن\\u200cها به\\u200cصورت هم\\u200cزمان پیش\\u200cبینی می\\u200cشوند', 'هر توکن یکی\\u200cیکی با شرط\\u200cگذاری روی زمینهٔ قبلی تولید می\\u200cشود', 'توکن\\u200cها به ترتیب معکوس پیش\\u200cبینی می\\u200cشوند', 'بدون استفاده از زمینه، به\\u200cصورت تصادفی انتخاب می\\u200cشوند']\nanswer: 1\n----------------------------------------\nQ003 کدام‌یک از موارد زیر ترکیب رایج یک بلوکِ ترنسفورمر را تشکیل می‌دهد؟\noptions: ['لایهٔ توجهِ چندسَر، شبکهٔ خوراک\\u200cرو به جلو و مراحل نرمال\\u200cسازی', 'لایهٔ کانولوشن، استخرینگ و تبدیل فوریه', 'شبکهٔ بازگشتی، LSTM و دراپ\\u200cاوت', 'ماتریس خروجی، تابع هزینه و کرنل']\nanswer: 0\n----------------------------------------\nQ004 وظیفهٔ بخشِ رمزنگاری ورودیِ پیش از ستون بلوک‌ها در ترنسفورمر چیست؟\noptions: ['تبدیل یک توکن به یک بردار با استفاده از ماتریسِ جاسازی و رمزگذاری موقعیت', 'تبدیل بردار خروجی نهایی به توکن\\u200cهای کلمه', 'اعمال نرمال\\u200cسازی روی خروجی\\u200cهای ستون بلوک\\u200cها', 'ذخیرهٔ تمامی توکن\\u200cها در یک حافظهٔ خارجی']\nanswer: 0\n----------------------------------------\nQ005 سر مدل‌سازی زبانیِ پس از ستون بلوک‌ها چه عملی انجام می‌دهد؟\noptions: ['گرفتن بردارِ خروجی آخر، نگاشت آن با یک ماتریسِ بازجاسازی و اعمال نرم\\u200cافزار روی واژگان برای تولید یک توکن', 'فشرده\\u200cسازی تمام بردارهای خروجی به یک بردار منفرد', 'جایگزینی تمام جاسازی\\u200cها با بردارهای ثابت', 'تولید مستقیم نمایش\\u200cهای موقعیتی جدید']\nanswer: 0\n----------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip -q install -U transformers accelerate datasets peft trl bitsandbytes","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-12-14T15:23:05.687291Z","iopub.execute_input":"2025-12-14T15:23:05.687589Z","iopub.status.idle":"2025-12-14T15:23:09.798615Z","shell.execute_reply.started":"2025-12-14T15:23:05.687564Z","shell.execute_reply":"2025-12-14T15:23:09.797611Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 3) Train/Test split","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/questions/questions.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nchunks = data[\"by_chunk\"]\n\nrng = random.Random(42)\nrng.shuffle(chunks)\n\nn_test_chunks = max(1, round(len(chunks) * 0.20))\ntest_chunks = chunks[:n_test_chunks]\ntrain_chunks = chunks[n_test_chunks:]\n\ndef flatten(chunks_list):\n    out = []\n    for c in chunks_list:\n        out.extend(c[\"questions\"])\n    return out\n\ntrain_qs = flatten(train_chunks)\ntest_qs = flatten(test_chunks)\n\nprint(\"train questions:\", len(train_qs))\nprint(\"test questions:\", len(test_qs))\nprint(\"test chunk_ids:\", [c[\"chunk_id\"] for c in test_chunks])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:23:13.998269Z","iopub.execute_input":"2025-12-14T15:23:13.999114Z","iopub.status.idle":"2025-12-14T15:23:14.012079Z","shell.execute_reply.started":"2025-12-14T15:23:13.999066Z","shell.execute_reply":"2025-12-14T15:23:14.011384Z"}},"outputs":[{"name":"stdout","text":"train questions: 90\ntest questions: 20\ntest chunk_ids: ['CHUNK_08', 'CHUNK_04']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def make_prompt(q):\n    opts = \"\\n\".join([f\"{i}) {q['options'][i]}\" for i in range(4)])\n    return (\n        \"You are taking a multiple-choice exam.\\n\"\n        \"Read the Persian question and options below.\\n\"\n        \"Return ONLY the index of the correct option as a single digit: 0, 1, 2, or 3.\\n\"\n        \"Do not output any other text.\\n\\n\"\n        f\"Question (Persian): {q['question']}\\n\"\n        f\"Options (Persian):\\n{opts}\\n\"\n        \"Answer:\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:23:17.764962Z","iopub.execute_input":"2025-12-14T15:23:17.765231Z","iopub.status.idle":"2025-12-14T15:23:17.769913Z","shell.execute_reply.started":"2025-12-14T15:23:17.765209Z","shell.execute_reply":"2025-12-14T15:23:17.769098Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 4) Compute baseline accuracy","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\" \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n)\nbase_model.eval()\n\ndef predict_choice(model, q):\n    prompt = make_prompt(q)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a strict exam grader. Output must be exactly one digit: 0, 1, 2, or 3. No other text.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=4, do_sample=False)\n\n    gen = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n    m = re.search(r\"[0-3]\", gen)\n    return (int(m.group()) if m else None), gen\n\ndef eval_accuracy(model, questions):\n    correct, invalid = 0, 0\n    for q in questions:\n        pred, raw = predict_choice(model, q)\n        if pred is None:\n            invalid += 1\n            continue\n        if pred == q[\"answer\"]:\n            correct += 1\n    total = len(questions)\n    acc = correct / total\n    return {\"accuracy\": acc, \"correct\": correct, \"total\": total, \"invalid\": invalid}\n\nbase_metrics = eval_accuracy(base_model, test_qs)\nbase_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:24:06.078403Z","iopub.execute_input":"2025-12-14T15:24:06.079466Z","iopub.status.idle":"2025-12-14T15:24:17.234928Z","shell.execute_reply.started":"2025-12-14T15:24:06.079431Z","shell.execute_reply":"2025-12-14T15:24:17.234218Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117af36f593245938f499c57b113214c"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.45, 'correct': 9, 'total': 20, 'invalid': 0}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"for i, q in enumerate(test_qs[6:10], 1):\n    pred, raw = predict_choice(base_model, q)\n    print('='*40)\n    print(\"Q:\", q[\"question\"])\n    for j, opt in enumerate(q[\"options\"]):\n        print(f\"  {j}) {opt}\")\n    print(\"GT (answer):\", q[\"answer\"])\n    print(\"Pred:\", pred)\n    print(\"Raw model output:\", repr(raw))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:24:29.211679Z","iopub.execute_input":"2025-12-14T15:24:29.212368Z","iopub.status.idle":"2025-12-14T15:24:30.861071Z","shell.execute_reply.started":"2025-12-14T15:24:29.212340Z","shell.execute_reply":"2025-12-14T15:24:30.860079Z"}},"outputs":[{"name":"stdout","text":"========================================\nQ: پس از محاسبهٔ ماتریس مقایسهٔ کوئری‌ها و کلیدها چه گام‌هایی پیش می‌آید تا بردار نمایش هر توکن تولید شود؟\n  0) مقیاس‌گذاری امتیازها، اعمال نرم‌افزار نرم‌سازی، و سپس ضرب در ماتریس مقدار\n  1) مستقیماً استفاده از ماتریس مقایسه به‌عنوان خروجی نهایی\n  2) جمع‌کردن کوئری و کلید و سپس ارسال به لایهٔ بازخور\n  3) اول ضرب در مقدار و سپس اعمال نرم‌افزار نرم‌سازی\nGT (answer): 0\nPred: 1\nRaw model output: '1'\n\n========================================\nQ: در توضیح نویسنده، ترتیب بررسی توجه به چه صورت است؟\n  0) ابتدا شرح چند سر، سپس تبدیل به تک‌سر\n  1) ابتدا یک سر توجه را بررسی کرده و بعد سرهای متعدد را مطرح می‌کنند\n  2) فقط حالت چند سر توضیح داده می‌شود و تک‌سر بررسی نمی‌شود\n  3) مستقیماً کل بلوک ترنسفورمر بدون اشاره به سرها شرح داده می‌شود\nGT (answer): 1\nPred: 1\nRaw model output: '1'\n\n========================================\nQ: در ماتریس X هر سطر نشان‌دهندهٔ چه چیزی است؟\n  0) بردار جاسازی یک توکن از ورودی\n  1) وزن‌های توجه بین دو توکن\n  2) مجموع کلیدها و کوئری‌ها\n  3) نتیجهٔ ضرب QK ترانهاده\nGT (answer): 0\nPred: 1\nRaw model output: '1'\n\n========================================\nQ: کاهش کل مرحلهٔ خودتوجه برای دنباله‌ای از توکن‌ها به ضرب ماتریسی چه مزیتی فراهم می‌آورد؟\n  0) حذف نیاز به لایه‌های باقیمانده\n  1) امکان محاسبهٔ هم‌زمان و کارا برای تمام توکن‌ها\n  2) جایگزینی نرم‌افزار نرم‌سازی با عملیات خطی ساده\n  3) کاهش تعداد توکن‌های ورودی مورد نیاز\nGT (answer): 1\nPred: 1\nRaw model output: '1'\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 5) Prepare data for supervised fine-tuning (SFT)","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nSYSTEM_MSG = \"You are a strict exam grader. Output must be exactly one digit: 0, 1, 2, or 3. No other text.\"\n\ndef to_record(q):\n    prompt = make_prompt(q)\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": str(q[\"answer\"])},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n    return {\"text\": text}\n\ntrain_ds = Dataset.from_list([to_record(q) for q in train_qs])\neval_ds  = Dataset.from_list([to_record(q) for q in test_qs])\n\ntrain_ds[0][\"text\"][:400]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:24:36.269672Z","iopub.execute_input":"2025-12-14T15:24:36.270211Z","iopub.status.idle":"2025-12-14T15:24:36.510336Z","shell.execute_reply.started":"2025-12-14T15:24:36.270185Z","shell.execute_reply":"2025-12-14T15:24:36.509631Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are a strict exam grader. Output must be exactly one digit: 0, 1, 2, or 3. No other text.<|im_end|>\\n<|im_start|>user\\nYou are taking a multiple-choice exam.\\nRead the Persian question and options below.\\nReturn ONLY the index of the correct option as a single digit: 0, 1, 2, or 3.\\nDo not output any other text.\\n\\nQuestion (Persian): وظیفه محاسبه توجه در یک لایه ترنسفورمر چیست؟\\nOp'"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## 6) QLoRA adapters","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:30:06.007197Z","iopub.execute_input":"2025-12-14T15:30:06.007780Z","iopub.status.idle":"2025-12-14T15:30:14.433797Z","shell.execute_reply.started":"2025-12-14T15:30:06.007756Z","shell.execute_reply":"2025-12-14T15:30:14.433033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e59dc413fb4c34bdacd8ccfd8443ab"}},"metadata":{}},{"name":"stdout","text":"trainable params: 59,867,136 || all params: 3,145,805,824 || trainable%: 1.9031\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 7) Train with SFTTrainer ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom trl import SFTTrainer, SFTConfig\n\nsft_args = SFTConfig(\n    output_dir=\"qwen25_mcq_lora\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    logging_steps=10,\n    save_steps=100,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    report_to=\"none\",\n    optim=\"paged_adamw_8bit\",\n    packing=False,\n    dataset_text_field=\"text\",\n    max_length=512,\n    bf16=torch.cuda.is_available(),\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    processing_class=tokenizer,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:30:46.960080Z","iopub.execute_input":"2025-12-14T15:30:46.960668Z","iopub.status.idle":"2025-12-14T15:38:10.883342Z","shell.execute_reply.started":"2025-12-14T15:30:46.960641Z","shell.execute_reply":"2025-12-14T15:38:10.882688Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/90 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"778a3afdeb70454796a411dba12097e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/90 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d28a336c859a493c960189257ce231be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/90 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8ccca1c4de4ab1bca2b6598db3487a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7ed20d028f45bc94c91140818e16df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac4755425b8418fa331b6e63451b202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe5867a75bc4096a16a2af28de8bd27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18/18 06:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=18, training_loss=1.1075519720713298, metrics={'train_runtime': 442.9266, 'train_samples_per_second': 0.61, 'train_steps_per_second': 0.041, 'total_flos': 1356987951169536.0, 'train_loss': 1.1075519720713298, 'entropy': 0.7836652048702898, 'num_tokens': 74412.0, 'mean_token_accuracy': 0.8233741377962047, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"qwen_finetuned= \"qwen_finetuned_r32\"\ntrainer.model.save_pretrained(qwen_finetuned)\ntokenizer.save_pretrained(qwen_finetuned)\nprint(\"Saved adapter to:\", qwen_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:38:14.158483Z","iopub.execute_input":"2025-12-14T15:38:14.158797Z","iopub.status.idle":"2025-12-14T15:38:14.779997Z","shell.execute_reply.started":"2025-12-14T15:38:14.158773Z","shell.execute_reply":"2025-12-14T15:38:14.779090Z"}},"outputs":[{"name":"stdout","text":"Saved adapter to: qwen_finetuned_r32\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 8) Final evaluation ","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\nft_base = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n)\nft_model = PeftModel.from_pretrained(ft_base, qwen_finetuned)\nft_model.eval()\n\nft_metrics = eval_accuracy(ft_model, test_qs)\nprint(\"BASE:\", base_metrics)\nprint(\"FT  :\", ft_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T15:38:25.036377Z","iopub.execute_input":"2025-12-14T15:38:25.036663Z","iopub.status.idle":"2025-12-14T15:48:59.799081Z","shell.execute_reply.started":"2025-12-14T15:38:25.036641Z","shell.execute_reply":"2025-12-14T15:48:59.798028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357a02f04b604fbfb55e24839df10b03"}},"metadata":{}},{"name":"stderr","text":"WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n","output_type":"stream"},{"name":"stdout","text":"BASE: {'accuracy': 0.45, 'correct': 9, 'total': 20, 'invalid': 0}\nFT  : {'accuracy': 0.8, 'correct': 16, 'total': 20, 'invalid': 0}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Metrics (Test Set)\n- **Test size:** 20 questions  \n\n| Model | Accuracy | Correct | Total |\n|------|----------|---------|-------|\n| Base | 0.45 | 9 | 20 | 0 |\n| Fine-tuned (LoRA) | 0.80 | 16 | 20 |\n\n- Fine-tuning with QLoRA improved accuracy from **45% → 80%** on the held-out test set .\n\n","metadata":{}}]}