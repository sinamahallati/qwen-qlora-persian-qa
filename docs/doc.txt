CHUNK 01

Transformers “The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759 In this chapter we introduce the transformer, the standard architecture for building large language models. As we discussed in the prior chapter, transformer-based large language models have completely changed the ﬁeld of speech and language processing. Indeed, every subsequent chapter in this textbook will make use of them. As with the previous chapter, we’ll focus for this chapter on the use of transformers to model left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer is a neural network with a speciﬁc structure that includes a mechanism called self-attention or multi-head attention. Attention can be thought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. A transformer has three major components. At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer

normalization steps) that maps an input vector xi in column i (corresponding to input token i) to an output vector hi. The set of n blocks maps an entire context window of input vectors (x1,...,xn) to a window of output vectors (h1,...,hn) of the same length. A column might contain from 12 to 96 or more stacked blocks. The column of blocks is preceded by the input encoding component, which processes an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding output by the ﬁnal transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column. Transformer-based language models are complex, and so the details will unfold over the next few chapters. Chapter 7 already discussed how language models are pretrained, and how tokens are generated via sampling. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components of the transformer. Chapter 10 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 9 shows how to instruction-tune language models to perform NLP tasks, and how to align the model with human preferences. Chapter 12 will introduce machine translation with the encoder-decoder architecture. We’ll see further use of the encoder-decoder architecture in Chapter 15. 8. Attention Recall from Chapter 5 that for word2vec and other static embeddings, the representation of a word’s meaning is always the same vector irrespective of the context: the word chicken, for example, is always represented by the same ﬁxed vector. So a static vector for the word it might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning. Consider it in one of these two sentences: (8.1) The chicken didn’t cross the road because it was too tired. (8.2) The chicken didn’t cross the road because it was too wide. In (8.1) it is the chicken (i.e., the reader knows that the chicken was tired), while in (8.2) it is the road (and the reader knows that the road was wide). That is, if we are to compute the meaning of this sentence, we’ll need the meaning of it to be associated with the chicken in the ﬁrst sentence and associated with the road in the second one, sensitive to the context. Furthermore, consider reading left to right like a causal language model, processing the sentence up to the word it: (8.3) The chicken didn’t cross the road because it At this point we don’t yet know which thing it is going to end up referring to! So a representation of it at this point might have aspects of both chicken and road as the reader is trying to guess what happens next. This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples: (8.4) The keys to the cabinet are on the table.

--------------------------------------------------------------------------------

CHUNK 02

8. ATTENTION (8.5) I walked along the pond, and noticed one of the trees along the bank. In (8.4), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can’t use a singular verb like is with a plural subject like keys (we’ll discuss agreement more in Chapter 18). In (8.5), we know that bank refers to the side of a pond or river and not a ﬁnancial institution because of the context, including words like pond. (We’ll discuss word senses more in Chapter 10.) The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a contextual embeddings transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for each word at each position. Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer k to build the representation for tokens in layer k +1. The chicken didn’t cross the road because it was too tired The chicken didn’t cross the road because it was too tired Layer k+1 Layer k self-attention distribution columns corresponding to input tokens The self-attention weight distribution α that is part of the computation of the representation for the word it at layer k +1. In computing the representation for it, we attend differently to the various words at layer k, with darker shades indicating higher self-attention values. Note that the transformer is attending highly to the columns corresponding to the tokens chicken and road , a sensible result, since at the point where it occurs, it could plausibly corefer with the chicken or the road, and hence we’d like the representation for it to draw on the representation for these earlier words. Figure adapted from Uszkoreit (2017). Fig. 8. shows a schematic example simpliﬁed from a transformer (Uszkoreit, 2017). The ﬁgure describes the situation when the current token is it and we need to compute a contextual representation for this token at layer k+1 of the transformer, drawing on the representations (from layer k) of every prior token. The ﬁgure uses color to represent the attention distribution over the contextual words: the tokens chicken and road both have a high attention weight, meaning that as we are computing the representation for it, we will draw most heavily on the representation for chicken and road. This will be useful in building the ﬁnal representation for it, since it will end up coreferring with either chicken or road. Let’s now turn to how this attention distribution is represented and computed.

--------------------------------------------------------------------------------

CHUNK 03

8.1. Attention more formally As we’ve said, the attention computation is a way to compute a vector representation for a token at a particular layer of a transformer, by selectively attending to and integrating information from prior tokens at the previous layer. Attention takes an input representation xi corresponding to the input token at position i, and a context window of prior inputs x1..xi−1, and produces an output ai. In causal, left-to-right language models, the context is any of the prior words. That is, when processing xi, the model has access to xi as well as the representations of all the prior tokens in the context window (context windows consist of thousands of tokens) but no tokens after i. (By contrast, in Chapter 10 we’ll generalize attention so it can also look ahead to future words.) Fig. 8. illustrates this ﬂow of information in an entire causal self-attention layer, in which this same attention computation happens in parallel at each token position i. Thus a self-attention layer maps input sequences (x1,...,xn) to output sequences of the same length (a1,...,an). attention attention Self-Attention attention attention attention a1 a2 a3 a4 a5 At its heart, attention is really just a weighted sum of context vectors, with a lot of complications added to how the weights are computed and what gets summed. For pedagogical purposes let’s ﬁrst describe a simpliﬁed intuition of attention, in which the attention output ai at token position i is simply the weighted sum of all the representations xj, for all j ≤i; we’ll use αij to mean how much xj should contribute to ai: Simpliﬁed version: ai = X j≤i (8.6) Each αi j is a scalar used for weighing the value of input xj when summing up the inputs to compute ai. How shall we compute this α weighting? In attention we weight each prior embedding proportionally to how similar it is to the current token i. So the output of attention is a sum of the embeddings of prior tokens weighted by their similarity with the current token embedding. We compute similarity scores via dot product, which maps two vectors into a scalar value ranging from −∞to ∞. The larger the score, the more similar the vectors that are being compared. We’ll normalize these scores with a softmax to create the vector of weights αij, j ≤i. Simpliﬁed Version: score(xi,xj) = xi ·xj (8.7) αij = softmax(score(xi,x j)) ∀j ≤i (8.8) Thus in Fig. 8. we compute a3 by computing three scores: x3 ·x1, x3 ·x2 and x3 ·x3, normalizing them by a softmax, and using the resulting probabilities as weights indicating each of their proportional relevance to the current position i. Of course,

--------------------------------------------------------------------------------

CHUNK 04

8. ATTENTION the softmax weight will likely be highest for xi, since xi is very similar to itself, resulting in a high dot product. But other context words may also be similar to i, and the softmax will also assign some weight to those words. Then we use these weights as the α values in Eq. 8. to compute the weighted sum that is our a3. The simpliﬁed attention in equations 8. – 8. demonstrates the attention-based approach to computing ai: compare the xi to prior vectors, normalize those scores into a probability distribution used to weight the sum of the prior vector. But now we’re ready to remove the simpliﬁcations. A single attention head using query, key, and value matrices Now that we’ve seen a simple intuition of attention, let’s introduce the actual attention head, the attention head version of attention that’s used in transformers. (The word head is often used in head transformers to refer to speciﬁc structured layers). The attention head allows us to distinctly represent three different roles that each input embedding plays during the course of the attention process: • As the current element being compared to the preceding inputs. We’ll refer to this role as a query. query • In its role as a preceding input that is being compared to the current element to determine a similarity weight. We’ll refer to this role as a key. key • And ﬁnally, as a value of a preceding element that gets weighted and summed value up to compute the output for the current element. To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will project each input vector xi into a representation of its role as a query, key, or value: qi = xiWQ; ki = xiWK; vi = xiWV (8.9) Given these projections, when we are computing the similarity of the current element xi with some prior element xj, we’ll use the dot product between the current element’s query vector qi and the preceding element’s key vector kj. Furthermore, the result of a dot product can be an arbitrarily large (positive or negative) value, and exponentiating large values can lead to numerical issues and loss of gradients during training. To avoid this, we scale the dot product by a factor related to the size of the embeddings, via dividing by the square root of the dimensionality of the query and key vectors (dk). We thus replace the simpliﬁed Eq. 8. with Eq. 8.11. The ensuing softmax calculation resulting in αij remains the same, but the output calculation for headi is now based on a weighted sum over the value vectors v (Eq. 8.13). Here’s a ﬁnal set of equations for computing self-attention for a single selfattention output vector ai from a single input vector xi. This version of attention computes ai by summing the values of the prior elements, each weighted by the similarity of its key to the query from the current element: qi = xiWQ; kj = xjWK; vj = xjWV (8.10) score(xi,xj) = qi ·kj (8.11) αij = softmax(score(xi,x j)) ∀j ≤i (8.12) headi = X j≤i (8.13) ai = headiWO (8.14)

--------------------------------------------------------------------------------

CHUNK 05

6. Sum the weighted value vectors a3 1. Generate key, query, value vectors k q v WK WQ WV 5. Weigh each value vector ÷ ÷ ÷ k q v WK WQ WV k q v WK WQ WV WO We illustrate this in Fig. 8. for the case of calculating the value of the third output a3 in a sequence. Note that we’ve also introduced one more matrix, WO, which is right-multiplied by the attention head. This is necessary to reshape the output of the head. The input to attention xi and the output from attention ai both have the same dimensionality [1 × d]. We often call d the model dimensionality, and indeed as we’ll discuss in Section 8. the output hi of each transformer block, as well as the intermediate vectors inside the transformer block also have the same dimensionality [1×d]. Having everything be the same dimensionality makes the transformer very modular. So let’s talk shapes. How do we get from [1 × d] at the input to [1 × d] at the output? Let’s look at all the internal shapes. We’ll have a dimension dk for the query and key vectors. The query vector and the key vector are both dimensionality [1 × dk], so we can take their dot product qi · kj to produce a scalar. We’ll have a separate dimension dv for the value vectors. The transform matrix WQ has shape [d × dk], WK is [d × dk], and WV is [d × dv]. So the output of headi in equation Eq. 8. is of shape [1 × dv]. To get the desired output shape [1 × d] we’ll need to reshape the head output, and so WO is of shape [dv ×d]. In the original transformer work (Vaswani et al., 2017), d was 512, dk and dv were both 64. Multi-head Attention Equations 8.11-8. describe a single attention head. But actually, transformers use multiple attention heads. The intuition is that each head might be attending to the context for different purposes: heads might be specialized to represent different linguistic relationships between context elements and the current token, or to look for particular kinds of patterns in the context. So in multi-head attention we have A separate attention heads that reside in multi-head attention parallel layers at the same depth in a model, each with its own set of parameters that allows the head to model different aspects of the relationships among inputs. Thus

--------------------------------------------------------------------------------

CHUNK 06

8. TRANSFORMER BLOCKS each head i in a self-attention layer has its own set of query, key, and value matrices: WQi, WKi, and WVi. These are used to project the inputs into separate query, key, and value embeddings for each head. When using multiple heads the model dimension d is still used for the input and output, the query and key embeddings have dimensionality dk, and the value embeddings are of dimensionality dv (again, in the original transformer paper dk = dv = 64, A = 8, and d = 512). Thus for each head i, we have weight layers WQi of shape [d ×dk], WKi of shape [d ×dk], and WVi of shape [d ×dv]. Below are the equations for attention augmented with multiple heads; Fig. 8. shows an intuition. qc i = xiWQc; kc j = xjWKc; vc j = xjWVc; ∀c 1 ≤c ≤A (8.15) scorec(xi,xj) = qc i ·kc j (8.16) ij = softmax(scorec(xi,xj)) ∀j ≤i (8.17) headc i = X j≤i ijvc j (8.18) ai = (head1 ⊕head2...⊕headA)WO (8.19) MultiHeadAttention(xi,[x1,··· ,xi−1]) = ai (8.20) Note in Eq. 8. that MultiHeadAttention is a function of the current input xi, as well as all the other inputs. For the causal or left-to-right attention that we use in this chapter, the other inputs are only to the left, but we’ll also see a version of attention in Chapter 10 where attention is a function of the tokens to the right as well. We’ll return to this idea about causal inputs in Eq. 8. when we introduce the idea of masking the right context. The output of each of the A heads is of shape [1 × dv], and so the output of the multi-head layer with A heads consists of A vectors of shape [1 × dv]. These are concatenated to produce a single output with dimensionality [1×Adv]. Then we use yet another linear projection WO ∈RAdv×d to reshape it, resulting in the multi-head attention vector ai with the correct output shape [1×d] at each input i. 8. Transformer Blocks The self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes three other kinds of layers: (1) a feedforward layer, (2) residual connections, and (3) normalizing layers (colloquially called “layer norm”). Fig. 8. illustrates a transformer block, sketching a common way of thinking about the block that is called the residual stream (Elhage et al., 2021). In the residresidual stream ual stream viewpoint, we consider the processing of an individual token i through the transformer block as a single stream of d-dimensional representations for token position i. This residual stream starts with the original input vector, and the various components read their input from the residual stream and add their output back into the stream. The input at the bottom of the stream is an embedding for a token, which has dimensionality d. This initial embedding gets passed up (by residual connections), and is progressively added to by the other components of the transformer: the at-

--------------------------------------------------------------------------------

CHUNK 07

8. TRANSFORMER BLOCKS We’ve already seen the attention layer, so let’s now introduce the feedforward and layer norm computations in the context of processing a single input xi at token position i. The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 6. The weights are the same for each token position i, but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and dff = 2048.) FFN(xi) = ReLU(xiW1 +b1)W2 +b2 (8.21) At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one layer norm of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layer. That is, the term layer norm is a bit confusing; layer norm is not applied to an entire transformer layer, but just to the embedding vector of a single token. Thus the input to layer norm is a single vector of dimensionality d and the output is that vector normalized, again of dimensionality d. The ﬁrst step in layer normalization is to calculate the mean, µ, and standard deviation, σ, over the elements of the vector to be normalized. Given an embedding vector x of dimensionality d, these values are calculated as follows. µ = 1 d d X i=1 xi (8.22) σ = v u u t1 d d X i=1 (xi −µ) (8.23) Given these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one. ˆx = (x−µ) σ (8.24) Finally, in the standard implementation of layer normalization, two learnable parameters, γ and β, representing gain and offset values, are introduced. LayerNorm(x) = γ (x−µ) σ +β (8.25) Putting it all together The function computed by a transformer block can be expressed by breaking it down with one equation for each component computation, using t (of shape [1 × d]) to stand for transformer and superscripts to demarcate

each computation inside the block: t1 i = LayerNorm(xi) (8.26) t2 i = MultiHeadAttention(t1 i ,  t1 1,··· ,t1 N  ) (8.27) t3 i = t2 i +xi (8.28) t4 i = LayerNorm(t3 i ) (8.29) t5 i = FFN(t4 i ) (8.30) hi = t5 i +t3 i (8.31) Notice that the only component that takes as input information from other tokens (other residual streams) is multi-head attention, which (as we see from Eq. 8.27) looks at all the neighboring tokens in the context. The output from attention, however, is then added into this token’s embedding stream. In fact, Elhage et al. (2021) show that we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream. The high-dimensional embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space. Fig. 8. shows a visualization of this movement. Token A residual stream Token B residual stream An attention head can move information from token A’s residual stream into token B’s residual stream. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Each token vector xi at the input to the block has dimensionality d, and the output hi also has dimensionality d. Transformers for large language models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small language models) to 96 layers (used for GPT-3 large), to even more for more recent models. We’ll come back to this issue of stacking in a bit. Equation 8. and following are just the equation for a single transformer block, but the residual stream metaphor goes through all the transformer layers, from the ﬁrst transformer blocks to the 12th, in a 12-layer transformer. At the earlier transformer blocks, the residual stream is representing the current token. At the highest transformer blocks, the residual stream is usually representing the following token, since at the very end it’s being trained to predict the next token. Once we stack many blocks, there is one more requirement: at the very end of the last (highest) transformer block, there is a single extra layer norm that is run on the last hi of each token stream (just below the language model head layer that we will deﬁne soon). Note that we are using the most common

--------------------------------------------------------------------------------

CHUNK 08

8. PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 8. Parallelizing computation using a single matrix X This description of multi-head attention and the rest of the transformer block has been from the perspective of computing a single output at a single time step i in a single residual stream. But as we pointed out earlier, the attention computation performed for each token to compute ai is independent of the computation for each other token, and that’s also true for all the computation in the transformer block computing hi from the input xi. That means we can easily parallelize the entire computation, taking advantage of efﬁcient matrix multiplication routines. We do this by packing the input embeddings for the N tokens of the input sequence into a single matrix X of size [N × d]. Each row of X is the embedding of one token of the input. Transformers for large language models commonly have an input length N from 1K to 32K; much longer contexts of 128K or even up to millions of tokens can also be achieved with architectural changes like special long-context mechanisms that we don’t discuss here. So for vanilla transformers, we can think of X having between 1K and 32K rows, each of the dimensionality of the embedding d (the model dimension). Parallelizing attention Let’s ﬁrst see this for a single attention head and then turn to multiple heads, and then add in the rest of the components in the transformer block. For one head we multiply X by the query, key, and value matrices WQ of shape [d ×dk], WK of shape [d ×dk], and WV of shape [d ×dv], to produce matrices Q of shape [N ×dk], K of shape [N ×dk], and V of shape [N ×dv], containing all the key, query, and value vectors: Q = XWQ; K = XWK; V = XWV (8.32) Given these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying Q and K⊺in a single matrix multiplication. The product is of shape N ×N, visualized in Fig. 8.8. q1•k1 q2•k1 q2•k2 q4•k1 q4•k2 q4•k3 q4•k4 q3•k1 q3•k2 q3•k3 N N q1•k2 q1•k3 q1•k4 q2•k3 q2•k4 q3•k4 The N × N QK⊺matrix showing how it computes all qi · k j comparisons in a single matrix multiple. Once we have this QK⊺matrix, we can very efﬁciently scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N ×d: a vector embedding representation for each token in the input. We’ve reduced the entire self-attention step for an entire sequence of N tokens for one head to the architecture.

--------------------------------------------------------------------------------

CHUNK 09

following computation: head = softmax  mask QK⊺  V (8.33) A = head WO (8.34) Masking out the future You may have noticed that we introduced a mask function in Eq. 8. above. This is because the self-attention computation as we’ve described it has a problem: the calculation of QK⊺results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling: guessing the next word is pretty simple if you already know it! To ﬁx this, the elements in the upper-triangular portion of the matrix are set to −∞, which the softmax will turn to zero, thus eliminating any knowledge of words that follow in the sequence. This is done in practice by adding a mask matrix M in which Mi j = −∞∀j > i (i.e. for the upper-triangular portion) and Mij = 0 otherwise. Fig. 8. shows the resulting masked QK⊺matrix. (we’ll see in Chapter 10 how to make use of words in the future for tasks that need it). q1•k1 q2•k1 q2•k2 q4•k1 q4•k2 q4•k3 q4•k4 q3•k1 q3•k2 q3•k3 N N −∞ −∞ −∞ −∞ −∞ −∞ The N ×N QK⊺matrix showing the qi ·kj values, with the upper-triangle portion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero). Fig. 8. shows a schematic of all the computations for a single attention head parallelized in matrix form. Fig. 8. and Fig. 8. also make it clear that attention is quadratic in the length of the input, since at each layer we need to compute dot products between each pair of tokens in the input. This makes it expensive to compute attention over very long documents (like entire novels). Nonetheless modern large language models manage to use quite long contexts of thousands or tens of thousands of tokens. Parallelizing multi-head attention In multi-head attention, as with self-attention, the input and output have the model dimension d, the key and query embeddings have dimensionality dk, and the value embeddings are of dimensionality dv (again, in the original transformer paper dk = dv = 64, A = 8, and d = 512). Thus for each head c, we have weight layers WQc of shape [d ×dk], WKc of shape [d ×dk], and WVc of shape [d ×dv], and these get multiplied by the inputs packed into X to produce Q of shape [N × dk], K of shape [N × dk], and V of shape [N × dv]. The output of each of the A heads is of shape [N × dv], and so the output of the multihead layer with A heads consists of A matrices of shape [N × dv]. To make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality [N × Adv]. Finally, we use a ﬁnal linear projection WO of shape [Adv ×d], that reshapes it to the original output dimension for each token. Multiplying the concatenated [N × Adv] matrix output by WO of shape [Adv × d]

8. PARALLELIZING COMPUTATION USING A SINGLE MATRIX X q1 q2 q3 q4 k1 k2 k3 k4 Q KT QKT v1 v2 v3 v4 V q2•k2 q4•k2 q4•k3 q4•k4 q3•k2 q3•k3 −∞ −∞ −∞ −∞ −∞ −∞ q1•k1 q2•k1 q2•k2 q4•k1 q4•k2 q4•k3 q4•k4 q3•k1 q3•k2 q3•k3 q1•k2 q2•k3 q1•k3 q3•k4 q2•k4 q1•k4 x = QKT masked mask = q1•k1 q2•k1 q4•k1 q3•k1 q1•k1 q1•k1 = x a1 a2 a3 a4 A Query Token 1 Query Token 2 Query Token 3 Query Token 4 Q Token 1 Token 2 Token 3 Token 4 X x WQ = Value Token 1 Value Token 2 Value Token 3 Value Token 4 V x WV = Token 1 Token 2 Token 3 Token 4 X Key Token 1 Key Token 2 Key Token 3 Key Token 4 K x WK = Token 1 Token 2 Token 3 Token 4 X N x dk dk x N N x N N x N N x dv N x dv d x dk d x dk d x dv N x d N x dk N x d N x dk N x d N x dv Qi = XWQi ; Ki = XWKi ; Vi = XWVi (8.35) headi = SelfAttention(Qi,Ki,Vi) = softmax  mask QiKi⊺  Vi (8.36) MultiHeadAttention(X) = (head1 ⊕head2...⊕headA)WO (8.37) Putting it all together with the parallel input matrix X The function computed in parallel by an entire layer of N transformer blocks—each block over one of the N input tokens—can be expressed as: O = X+MultiHeadAttention(LayerNorm(X)) (8.38) H = O+FFN(LayerNorm(O)) (8.39) Note that in Eq. 8. we are using X to mean the input to the layer, wherever it comes from. For the ﬁrst layer, as we will see in the next section, that input is the initial word + positional embedding vectors that we have been describing by X. But for subsequent layers k, the input is the output from the previous layer Hk−1. We can also break down the computation performed in a transformer layer, showing one equation for each component computation. We’ll use T (of shape [N ×d]) to stand for transformer and superscripts to demarcate each computation inside the block, and again use X to mean the input to the block from the previous layer or the initial

--------------------------------------------------------------------------------

CHUNK 10

embedding: T1 = LayerNorm(X) (8.40) T2 = MultiHeadAttention(T1) (8.41) T3 = T2 +X (8.42) T4 = LayerNorm(T3) (8.43) T5 = FFN(T4) (8.44) H = T5 +T3 (8.45) Here when we use a notation like FFN(T3) we mean that the same FFN is applied in parallel to each of the N embedding vectors in the window. Similarly, each of the N tokens is normed in parallel in the LayerNorm. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Since each token xi at the input to the block is represented by an embedding of dimensionality [1×d], that means the input X and output H are both of shape [N ×d]. 8. The input: embeddings for token and position Let’s talk about where the input X comes from. Given a sequence of N tokens (N is the context length in tokens), the matrix X of shape [N × d] has an embedding for embedding each word in the context. The transformer does this by separately computing two embeddings: an input token embedding, and an input positional embedding. A token embedding, introduced in Chapter 6, is a vector of dimension d that will be our initial representation for the input token. (As we pass vectors up through the transformer layers in the residual stream, this embedding representation will change and grow, incorporating context and playing a different role depending on the kind of language model we are building.) The set of initial embeddings are stored in the embedding matrix E, which has a row for each of the |V| tokens in the vocabulary. (Reminder that V here means the vocabulary of tokens, this V is not related to the value vector.) Thus each word is a row vector of d dimensions, and E has shape Given an input token string like Thanks for all the we ﬁrst convert the tokens into vocabulary indices (these were created when we ﬁrst tokenized the input using BPE or SentencePiece). So the representation of thanks for all the might be w = [5,4000,10532,2224]. Next we use indexing to select the corresponding rows from E, (row 5, row 4000, row 10532, row 2224). Another way to think about selecting token embeddings from the embedding matrix is to represent tokens as one-hot vectors of shape [1 × |V|], i.e., with one dimension for each word in the vocabulary. Recall that in a one-hot vector all the one-hot vector elements are 0 except one, the element whose dimension is the word’s index in the vocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary, x5 = 1, and xi = 0 ∀i ̸= 5, as shown here: 1 2 3 4 5 6 7 ... Multiplying by a one-hot vector that has only one non-zero element xi = 1 simply selects out the relevant row vector for word i, resulting in the embedding for word i, as depicted in Fig. 8.11.

We can extend this idea to represent the entire token sequence as a matrix of onehot vectors, one for each of the N positions in the transformer’s context window, as shown in Fig. 8.12. d d N = ✕ N … 0 0 1 0 0 0 0 0 1 0 0 … 0 0 0 0 1 0 0 0 0 0 0 … 1 0 0 … 0 0 0 0 … embeddings speciﬁc to each position in an input sequence. positional embeddings Where do we get these positional embeddings? The simplest method, called absolute position, is to start with randomly initialized embeddings corresponding absolute position to each possible input position up to some maximum length. For example, just as we have an embedding for the word ﬁsh, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. We can store them in a matrix Epos of shape [N ×d]. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. The individual token and position embeddings are both of size [1×d], so their sum is also [1×d], This new embedding serves as the input for further processing. Fig. 8. shows the idea. X = Composite Embeddings (word + position) Transformer Block Janet will back Janet will back the bill the bill + + + + + Position Embeddings Embeddings A simple way to model position: add an embedding of the absolute position to the token embedding to produce a new embedding of the same dimensionality.

--------------------------------------------------------------------------------

CHUNK 11

The ﬁnal representation of the input, the matrix X, is an [N ×d] matrix in which each row i is the representation of the ith token in the input, computed by adding E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i], the positional embedding of position i. A potential problem with the simple position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative is to choose a static function that maps integer inputs to real-valued vectors in a way that better handles sequences of arbitrary length. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Sinusoidal position embeddings may also help in capturing the inherent relationships among the positions, like the fact that position 4 in an input is more closely related to position 5 than it is to position 17. A more complex style of positional embedding methods extend this idea of capturing relationships even further to directly represent relative position instead of relative position absolute position, often implemented in the attention mechanism at each layer rather than being added once at the initial input. 8. The last component of the transformer we must introduce is the language modeling head. Here we are using the word head to mean the additional neural circuitry we language modeling head head add on top of the basic transformer architecture when we apply pretrained transformer models to various tasks. The language modeling head is the circuitry we need to do language modeling. Recall that language models, from the simple n-gram models of Chapter 3 through the feedforward and RNN language models of Chapter 6 and Chapter 13, are word predictors. Given a context of words, they assign a probability to each possible next word. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “ﬁsh” we would compute: Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. The n-gram language models of Chapter 3 compute the probability of a word given counts of its occurrence with the n −1 prior words. The context is thus of size n −1. For transformer language models, the context is the size of the transformer’s context window, which can be quite large, like 32K tokens for large models (and much larger contexts of millions of words are possible with special long-context architectures). The job of the language modeling head is to take the output of the ﬁnal transformer layer from the last token N and use it to predict the upcoming word at position N +1. Fig. 8. shows how to accomplish this task, taking the output of the last token at the last layer (the d-dimensional output embedding of shape [1 × d]) and producing a probability distribution over words (from which we will choose one to generate). The ﬁrst module in Fig. 8. is a linear layer, whose job is to project from the output hL N, which represents the output token embedding at position N from the ﬁnal

--------------------------------------------------------------------------------

CHUNK 12

8. THE LANGUAGE MODELING HEAD Layer L Block … hL w1 w2 wN hL hL N 1 x d U = ET y1 y2 … u1 u2 … Language Model Head takes hL N and outputs a distribution over vocabulary V The language modeling head: the circuit at the top of a transformer that maps from the output embedding for token N from the last transformer layer (hL N) to a probability distribution over words in the vocabulary V. block L, (hence of shape [1×d]) to the logit vector, or score vector, that will have a logit single score for each of the |V| possible words in the vocabulary V. The logit vector This linear layer can be learned, but more commonly we tie this matrix to (the transpose of) the embedding matrix E. Recall that in weight tying, we use the weight tying same weights for two different matrices in the model. Thus at the input stage of the transformer the embedding matrix (of shape [|V|×d]) is used to map from a one-hot vector over the vocabulary (of shape [1 × |V|]) to an embedding (of shape [1 × d]). And then in the language model head, ET, the transpose of the embedding matrix (of shape [d ×|V|]) is used to map back from an embedding (shape [1×d]) to a vector over the vocabulary (shape [1×|V|]). In the learning process, E will be optimized to be good at doing both of these mappings. We therefore sometimes call the transpose ET the unembedding layer because it is performing this reverse mapping. unembedding A softmax layer turns the logits u into the probabilities y over the vocabulary. u = hL N ET (8.46) y = softmax(u) (8.47) We can use these probabilities to do things like help assign a probability to a given text. But the most important usage is to generate text, which we do by sampling a word from these probabilities y. We might sample the highest probability word (‘greedy’ decoding), or use another of the sampling methods from Section ?? or Section 8.6. In either case, whatever entry yk we choose from the probability vector y, we generate the word that has that index k. Fig. 8. shows the total stacked architecture for one token i. Note that the input to each transformer layer xℓ i is the same as the output from the preceding layer hℓ−1 i . A terminological note before we conclude: You will sometimes see a transformer used for this kind of unidirectional causal language model called a decoderonly model. This is because this model constitutes roughly half of the encoderdecoder-only model decoder model for transformers that we’ll see how to apply to machine translation in Chapter 12. (Confusingly, the original introduction of the transformer had an encoder-decoder architecture, and it was only later that the standard paradigm for

wi generate at position i+1 feedforward layer norm attention layer norm Input token i + … feedforward layer norm attention layer norm Layer 1 Layer 2 h1 i = x2 i i h2 i = x3 i feedforward layer norm attention layer norm hL i hL-1 i = xL i y1 y2 … Token probabilities u1 u2 … softmax wi+1 Layer L A transformer language model (decoder-only), stacking transformer blocks and mapping from an input token wi to to a predicted next token wi+1. causal language model was deﬁned by using only the decoder part of this original architecture). 8. More on Sampling The sampling methods we introduce below each have parameters that enable trading off two important factors in generation: quality and diversity. Methods that emphasize the most probable words tend to produce generations that are rated by people as more accurate, more coherent, and more factual, but also more boring and more repetitive. Methods that give a bit more weight to the middle-probability words tend to be more creative and more diverse, but less factual and more likely to be incoherent or otherwise low-quality. 8.6. Top-k sampling Top-k sampling is a simple generalization of greedy decoding. Instead of choosing top-k sampling the single most probable word to generate, we ﬁrst truncate the distribution to the

--------------------------------------------------------------------------------

CHUNK 13

8. TRAINING top k most likely words, renormalize to produce a legitimate probability distribution, and then randomly sample from within these k words according to their renormalized probabilities. More formally: 1. Choose in advance a number of words k 2. For each word in the vocabulary V, use the language model to compute the 3. Sort the words by their likelihood, and throw away any word that is not one of the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability. When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger number than 1 leads us to sometimes select a word which is not necessarily the most probable, but is still probable enough, and whose choice results in generating more diverse but still high-enough-quality text. 8.6. Nucleus or top-p sampling One problem with top-k sampling is that k is ﬁxed, but the shape of the probability distribution over words differs in different contexts. If we set k = 10, sometimes the top 10 words will be very likely and include most of the probability mass, but other times the probability distribution will be ﬂatter and the top 10 words will only include a small part of the probability mass. An alternative, called top-p sampling or nucleus sampling (Holtzman et al., top-p sampling 2020), is to keep not the top k words, but the top p percent of the probability mass. The goal is the same; to truncate the distribution to remove the very unlikely words. But by measuring probability rather than the number of words, the hope is that the measure will be more robust in very different contexts, dynamically increasing and decreasing the pool of word candidates. Given a distribution P(wt|w<t), we sort the distribution from most probable, and then the top-p vocabulary V (p) is the smallest set of words such that X w∈V (p) P(w|w<t) ≥p. (8.48) 8. Training We described the training process for language models in the prior chapter. Recall that large language models are trained with cross-entropy loss, also called the negative log likelihood loss. At time t the cross-entropy loss is the negative log probability the model assigns to the next word in the training sequence, −log p(wt+1). Fig. 8. illustrates the general training approach. At each step, given all the preceding words, the ﬁnal transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word by the model is used to calculate the cross-entropy loss for each item in the sequence. The loss for a training sequence is the average cross-entropy loss over the entire sequence. The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent.

--------------------------------------------------------------------------------

CHUNK 14

8. DEALING WITH SCALE as a power-law with each of these three properties of model training. For example, Kaplan et al. (2020) found the following three relationships for loss L as a function of the number of non-embedding parameters N, the dataset size D, and the compute budget C, for models training with limited parameters, dataset, or compute budget, if in each case the other two properties are held constant: L(N) = Nc N (8.49) L(D) = Dc D (8.50) L(C) = Cc C (8.51) The number of (non-embedding) parameters N can be roughly computed as follows (ignoring biases, and with d as the input and output dimensionality of the model, dattn as the self-attention layer size, and dff the size of the feedforward layer): N ≈2 d nlayer(2 dattn +dff) ≈12 nlayer d2 (8.52) (assuming dattn = dff/4 = d) Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × 122882 ≈175 billion parameters. The values of Nc, Dc, Cc, αN, αD, and αC depend on the exact transformer architecture, tokenization, and vocabulary size, so rather than all the precise values, scaling laws focus on the relationship with loss. Scaling laws can be useful in deciding how to train a model to a particular performance, for example by looking at early in the training curve, or performance with smaller amounts of data, to predict what the loss would be if we were to add more data or increase model size. Other aspects of scaling laws can also tell us how much data we need to add when scaling up a model. 8.8. KV Cache We saw in Fig. 8. and in Eq. 8. (repeated below) how the attention vector can be very efﬁciently computed in parallel for training, via two matrix multiplications: A = softmax QK⊺  V (8.53) Unfortunately we can’t do quite the same efﬁcient computation in inference as in training. That’s because at inference time, we iteratively generate the next tokens one at a time. For a new token that we have just generated, call it xi, we need to compute its query, key, and values by multiplying by WQ, WK, and WV respectively. But it would be a waste of computation time to recompute the key and value vectors for all the prior tokens x<i; at prior steps we already computed these key and value vectors! So instead of recomputing these, whenever we compute the key and value vectors we store them in memory in the KV cache, and then we can just KV cache grab them from the cache when we need them. Fig. 8. modiﬁes Fig. 8. to show

q4 k1 k2 k4 Q KT QKT v1 v2 v3 v4 V q4•k1 q4•k2 q4•k3 q4•k4 x = = x a4 A 1 x dk dk x N 1 x N N x dv 1 x dv k3 can take from the cache rather than recompute. 8.8. Parameter Efﬁcient Fine Tuning As we mentioned above, it’s very common to take a language model and give it more information about a new domain by ﬁnetuning it (continuing to train it to predict upcoming words) on some additional data. Fine-tuning can be very difﬁcult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes ﬁnetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be ﬁnetuned without changing all the parameters. Such methods are called parameter-efﬁcient ﬁne tuning or sometimes PEFT, because we efﬁciently select a subset of parameters parameterefﬁcient ﬁne tuning PEFT to update when ﬁnetuning. For example we freeze some of the parameters (don’t change them), and only update some particular subset of parameters. Here we describe one such model, called LoRA, for Low-Rank Adaptation. The LoRA intuition of LoRA is that transformers have many dense layers which perform matrix multiplication (for example the WQ, WK, WV, WO layers in the attention computation). Instead of updating these layers during ﬁnetuning, with LoRA we freeze these layers and instead update a low-rank approximation that has fewer parameters. Consider a matrix W of dimensionality [N ×d] that needs to be updated during ﬁnetuning via gradient descent. Normally this matrix would get updates ∆W of dimensionality [N ×d], for updating the N ×d parameters after gradient descent. In LoRA, we freeze W and update instead a low-rank decomposition of W. We create two matrices A and B, where A has size [N ×r] and B has size [r×d], and we choose r to be quite small, r << min(d,N). During ﬁnetuning we update A and B instead of W. That is, we replace W + ∆W with W + AB. Fig. 8. shows the intuition. For replacing the forward pass h = xW, the new forward pass is instead: h = xW +xAB (8.54) LoRA has a number of advantages. It dramatically reduces hardware requirements, since gradients don’t have to be calculated for most parameters. The weight updates can be simply added in to the pretrained weights, since AB is the same size as W). 076, Nc = 8. ×1013 (parameters), αD = 0.095, Dc = 5. ×1013 (tokens), αC = 0.050, Cc = 3. ×108 (petaﬂop-days).

--------------------------------------------------------------------------------

CHUNK 15

8. INTERPRETING THE TRANSFORMER h Pretrained Weights W N d r d A B r x d d N The intuition of LoRA. We freeze W to its pretrained values, and instead ﬁnetune by training a pair of matrices A and B, updating those instead of W, and just sum W and the updated AB. That means it doesn’t add any time during inference. And it also means it’s possible to build LoRA modules for different domains and just swap them in and out by adding them in or subtracting them from W. In its original version LoRA was applied just to the matrices in the attention computation (the WQ, WK, WV, and WO layers). Many variants of LoRA exist. 8. Interpreting the Transformer How does a transformer-based language model manage to do so well at language tasks? The subﬁeld of interpretability, sometimes called mechanistic interpretabilinterpretability ity, focuses on ways to understand mechanistically what is going on inside the transformer. In the next two subsections we discuss two well-studied aspects of transformer interpretability. 8.9. In-Context Learning and Induction Heads As a way of getting a model to do what we want, we can think of prompting as being fundamentally different than pretraining. Learning via pretraining means updating the model’s parameters by using gradient descent according to some loss function. But prompting with demonstrations can teach a model to do a new task. The model is learning something about the task from those demonstrations as it processes the prompt. Even without demonstrations, we can think of the process of prompting as a kind of learning. For example, the further a model gets in a prompt, the better it tends to get at predicting the upcoming tokens. The information in the context is helping give the model more predictive power. The term in-context learning was ﬁrst proposed by Brown et al. (2020) in their in-context learning introduction of the GPT3 system, to refer to either of these kinds of learning that lan-

guage models do from their prompts. In-context learning means language models learning to do new tasks, better predict tokens, or generally reduce their loss during the forward-pass at inference-time, without any gradient-based updates to the model’s parameters. How does in-context learning work? While we don’t know for sure, there are some intriguing ideas. One hypothesis is based on the idea of induction heads induction heads (Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit, which is a kind of abstract component of a network. The induction head circuit is part of the attention computation in transformers, discovered by looking at mini language models with only 1-2 attention heads. The function of the induction head is to predict repeated sequences. For example if it sees the pattern AB...A in an input sequence, it predicts that B will follow, instantiating the pattern completion rule AB...A→B. It does this by having a preﬁx matching component of the attention computation that, when looking at the current token A, searches back over the context to ﬁnd a prior instance of A. If it ﬁnds one, the induction head has a copying mechanism that “copies” the token B that followed the earlier A, by increasing the probability the B will occur next. Fig. 8. shows an example. An induction head looking at vintage uses the preﬁx matching mechanism to ﬁnd a prior instance of vintage, and the copying mechanism to predict that cars will occur again. Figure from Crosbie and Shutova (2022). Olsson et al. (2022) propose that a generalized fuzzy version of this pattern completion rule, implementing a rule like A*B*...A→B, where A* ≈A and B* ≈B (by ≈we mean they they are semantically similar in some way), might be responsible for in-context learning. Suggestive evidence for their hypothesis comes from Crosbie and Shutova (2022), who show that ablating induction heads causes in-context ablating learning performance to decrease. Ablation is originally a medical term meaning the removal of something. We use it in NLP interpretability studies as a tool for testing causal effects; if we knock out a hypothesized cause, we would expect the effect to disappear. Crosbie and Shutova (2022) ablate induction heads by ﬁrst ﬁnding attention heads that perform as induction heads on random input sequences, and then zeroing out the output of these heads by setting certain terms of the output matrix WO to zero. Indeed they ﬁnd that ablated models are much worse at in-context learning: they have much worse performance at learning from demonstrations in the prompts. 8.9. Logit Lens Another useful interpretability tool, the logit lens (Nostalgebraist, 2020), offers a logit lens way to visualize what the internal layers of the transformer might be representing. The idea is that we take any vector from any layer of the transformer and, pretending that it is the preﬁnal embedding, simply multiply it by the unembedding layer to get logits, and compute a softmax to see the distribution over words that that vector might be representing. This can be a useful window into the internal representations of the model. Since the network wasn’t trained to make the internal

--------------------------------------------------------------------------------

CHUNK 16

8. SUMMARY representations function in this way, the logit lens doesn’t always work perfectly, but this can still be a useful trick to help us visualize the internal layers of a transformer. 8. Summary This chapter has introduced the transformer and its components for the language modeling task introduced in the previous chapter. Here’s a summary of the main points that we covered: • Transformers are non-recurrent networks based on multi-head attention, a kind of self-attention. A multi-head attention computation takes an input vector xi and maps it to an output ai by adding in vectors from prior tokens, weighted by how relevant they are for the processing of the current word. • A transformer block consists of a residual stream in which the input from the prior layer is passed up to the next layer, with the output of different components added to it. These components include a multi-head attention layer followed by a feedforward layer, each preceded by layer normalizations. Transformer blocks are stacked to make deeper and more powerful networks. • The input to a transformer is computed by adding an embedding (computed with an embedding matrix) to a positional encoding that represents the sequential position of the token in the window. • Language models can be built out of stacks of transformer blocks, with a language model head at the top, which applies an unembedding matrix to the output H of the top layer to generate the logits, which are then passed through a softmax to generate word probabilities. • Transformer-based language models have a wide context window (200K tokens or even more for very large models with special mechanisms) allowing them to draw on enormous amounts of context to predict upcoming words. • There are various computational tricks for making large language models more efﬁcient, such as the KV cache and parameter-efﬁcient ﬁnetuning. Historical Notes The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks. Encoder-decoder attention, the idea of using a soft weighting over the encodings of input words to inform a generative decoder (see Chapter 12) was developed by Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended to self-attention by dropping the need for separate encoding and decoding sequences and instead seeing attention as a way of weighting the tokens in collecting information passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016). Other aspects of the transformer, including the terminology of key, query, and value, came from memory networks, a mechanism for adding an external readwrite memory to networks, by using an embedding of a query to match keys rep-

resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014). MORE HISTORY TBD IN NEXT DRAFT.

--------------------------------------------------------------------------------

