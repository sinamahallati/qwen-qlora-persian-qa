{
  "total_questions": 110,
  "chunks_used": 11,
  "questions": [
    {
      "id": "Q001",
      "question": "مکانیسمی که در ترنسفورمر برای ساختن نمایش‌های متنیِ متن-محور یک توکن استفاده می‌شود کدام است؟",
      "options": [
        "توجهِ خودی یا چندسَرِ توجه",
        "پردازش کانولوشنی",
        "نمونه‌برداری تصادفی",
        "شبکهٔ بازگشتی کلاسیک"
      ],
      "answer": 0
    },
    {
      "id": "Q002",
      "question": "در مدل‌سازی زبانیِ چپ‌به‌راست (خودِ علی)، پیش‌بینی توکن‌ها به چه صورت انجام می‌شود؟",
      "options": [
        "همهٔ توکن‌ها به‌صورت هم‌زمان پیش‌بینی می‌شوند",
        "هر توکن یکی‌یکی با شرط‌گذاری روی زمینهٔ قبلی تولید می‌شود",
        "توکن‌ها به ترتیب معکوس پیش‌بینی می‌شوند",
        "بدون استفاده از زمینه، به‌صورت تصادفی انتخاب می‌شوند"
      ],
      "answer": 1
    },
    {
      "id": "Q003",
      "question": "کدام‌یک از موارد زیر ترکیب رایج یک بلوکِ ترنسفورمر را تشکیل می‌دهد؟",
      "options": [
        "لایهٔ توجهِ چندسَر، شبکهٔ خوراک‌رو به جلو و مراحل نرمال‌سازی",
        "لایهٔ کانولوشن، استخرینگ و تبدیل فوریه",
        "شبکهٔ بازگشتی، LSTM و دراپ‌اوت",
        "ماتریس خروجی، تابع هزینه و کرنل"
      ],
      "answer": 0
    },
    {
      "id": "Q004",
      "question": "وظیفهٔ بخشِ رمزنگاری ورودیِ پیش از ستون بلوک‌ها در ترنسفورمر چیست؟",
      "options": [
        "تبدیل یک توکن به یک بردار با استفاده از ماتریسِ جاسازی و رمزگذاری موقعیت",
        "تبدیل بردار خروجی نهایی به توکن‌های کلمه",
        "اعمال نرمال‌سازی روی خروجی‌های ستون بلوک‌ها",
        "ذخیرهٔ تمامی توکن‌ها در یک حافظهٔ خارجی"
      ],
      "answer": 0
    },
    {
      "id": "Q005",
      "question": "سر مدل‌سازی زبانیِ پس از ستون بلوک‌ها چه عملی انجام می‌دهد؟",
      "options": [
        "گرفتن بردارِ خروجی آخر، نگاشت آن با یک ماتریسِ بازجاسازی و اعمال نرم‌افزار روی واژگان برای تولید یک توکن",
        "فشرده‌سازی تمام بردارهای خروجی به یک بردار منفرد",
        "جایگزینی تمام جاسازی‌ها با بردارهای ثابت",
        "تولید مستقیم نمایش‌های موقعیتی جدید"
      ],
      "answer": 0
    },
    {
      "id": "Q006",
      "question": "اگر هنگام خواندن جمله از چپ به راست هنوز مرجعِ ضمیر «it» روشن نشده باشد، نمایش آن در مدل چه ویژگی‌ای دارد؟",
      "options": [
        "فقط صفت‌های مربوط به یکی از مراجع ممکن را نشان می‌دهد",
        "ممکن است ترکیبی از ویژگی‌های هر دو مرجعِ احتمالی را در خود داشته باشد",
        "کاملاً برابر با بردارِ ثابت یک ضمیر است",
        "به‌طور کامل حذف و نادیده گرفته می‌شود"
      ],
      "answer": 1
    },
    {
      "id": "Q007",
      "question": "در مقایسه با جاسازی‌های ایستا مانند word2vec، نمایش‌های متنی مبتنی بر توجه چه تفاوتی دارند؟",
      "options": [
        "نمایش‌ها به متن پیرامون حساس هستند و با زمینه تغییر می‌کنند",
        "همیشه یک بردار ثابت و بدون تغییر ارائه می‌دهند",
        "تنها برای کلماتِ اسمی کار می‌کنند",
        "فقط برای زبان‌های با دستور ثابت قابل استفاده‌اند"
      ],
      "answer": 0
    },
    {
      "id": "Q008",
      "question": "ستونِ بلاک‌های ترنسفورمر چه نگاشتی را بین بردارهای ورودی و خروجی انجام می‌دهد؟",
      "options": [
        "پنجره‌ای از بردارهای ورودی را به پنجره‌ای از بردارهای خروجی با همان طول نگاشت می‌کند",
        "تمام بردارهای ورودی را به یک بردار خروجی واحد فشرده می‌کند",
        "تعداد بردارهای خروجی را افزایش می‌دهد تا طول را دو برابر کند",
        "فقط اولین بردار ورودی را به خروجی نگاشت می‌کند"
      ],
      "answer": 0
    },
    {
      "id": "Q009",
      "question": "پهنای معمولیِ ستونِ بلوک‌ها در یک ترنسفورمر چند بلوک متوالی ممکن است باشد؟",
      "options": [
        "حدود دوازده تا نود‌وشش یا بیشتر",
        "همیشه فقط یک بلوک",
        "چند هزار بلوک به طور معمول",
        "دو تا چهار بلوک معمولاً"
      ],
      "answer": 0
    },
    {
      "id": "Q010",
      "question": "مزیت اصلی مکانیزمِ توجه در پردازش زبان چیست؟",
      "options": [
        "ایجاد نمایش‌های زمینه‌ای با یکپارچه‌سازی اطلاعات از توکن‌های دوردست",
        "افزایش سرعتِ اجرای سخت‌افزار بدون بهبود نمایش‌ها",
        "حذف نیاز به هرگونه جاسازیِ ورودی",
        "محدود کردن مدل به روابطِ فقط محلی بین کلمات"
      ],
      "answer": 0
    },
    {
      "id": "Q011",
      "question": "در مدل ترنسفورمر، نقش مکانیزم attention چیست؟",
      "options": [
        "ترجمهٔ توکن‌ها به زبان دیگر",
        "حذف توکن‌‌های غیرمرتبط از ورودی",
        "وزن‌دهی و ترکیب نمایش‌های توکن‌های مناسب از متن برای ساخت نمایش در لایه بعد",
        "ذخیرهٔ مکان دقیق هر توکن"
      ],
      "answer": 2
    },
    {
      "id": "Q012",
      "question": "براساس متن، چگونه می‌توان فهمید که واژه bank به معنی کنارهٔ رود/برکه است نه مؤسسهٔ مالی؟",
      "options": [
        "با توجه به کلمات زمینه‌ای مثل pond که معنا را روشن می‌کنند",
        "با بررسی قاعدهٔ صرفی-نحوی",
        "با نگاه به زمان فعل جمله",
        "با توجه به علامت‌گذاری جمله"
      ],
      "answer": 0
    },
    {
      "id": "Q013",
      "question": "چگونه در ترنسفورمرها نمایش‌های زمینه‌ای واژه‌ها به تدریج ساخته می‌شود؟",
      "options": [
        "با استفاده از بردار ثابتی که برای هر واژه همیشه یکسان است",
        "با افزودن اطلاعات از لایهٔ قبلی و ترکیب آن با اطلاعات توکن‌های همسایه در هر لایه",
        "با شمارش فراوانی کلمات در متن و ایجاد بردار از فرکانس",
        "با برچسب‌گذاری دستی معانی هر واژه"
      ],
      "answer": 1
    },
    {
      "id": "Q014",
      "question": "در مثال جمله دربارهٔ it، ترنسفورمر بیشترین توجه را به کدام توکن‌ها معطوف می‌کند؟",
      "options": [
        "pond و bank",
        "keys و are",
        "subject و verb",
        "chicken و road"
      ],
      "answer": 3
    },
    {
      "id": "Q015",
      "question": "چرا منطقی است که مدل به توکن‌های chicken و road توجه بالایی داشته باشد وقتی می‌خواهد نمایش it را بسازد؟",
      "options": [
        "چون it ممکن است به chicken یا به road ارجاع داده شود",
        "چون این کلمات املا یا شکل نوشتاری مشابهی دارند",
        "چون آن‌ها تنها اسم‌های جمله هستند",
        "چون این‌ها نزدیک‌ترین توکن‌ها به it هستند"
      ],
      "answer": 0
    },
    {
      "id": "Q016",
      "question": "در شکل توضیحی، سایهٔ تیره‌تر روی ستون‌های توجه نشانگر چه چیزی است؟",
      "options": [
        "نشانگر میزان پایین‌تر توجه",
        "نشانگر مقادیر بالاترِ وزن توجه خودی",
        "نشانگر تکرار بیشتر آن توکن در متن",
        "نشانگر فاصلهٔ مکانی بیشتر از توکن هدف"
      ],
      "answer": 1
    },
    {
      "id": "Q017",
      "question": "در هر لایه، نمایشِ یک توکن i چگونه محاسبه می‌شود؟",
      "options": [
        "با تولید یک برچسب نحوی کلی برای کل جمله",
        "با ایجاد ترجمه ماشینی از همان توکن",
        "با ترکیب اطلاعات قبلی خودِ توکن i و اطلاعاتی که از توکن‌های همسایه دریافت می‌شود",
        "با اختصاص یک بردار ثابت مستقل از متن"
      ],
      "answer": 2
    },
    {
      "id": "Q018",
      "question": "متن چه می‌گوید دربارهٔ فاصلهٔ کلمات زمینه‌ای که در فهم معنای یک واژه کمک می‌کنند؟",
      "options": [
        "آن‌ها همیشه کلمات کناری و مجاور هستند",
        "فقط در همان جملهٔ بلافصل قابل استفاده‌اند",
        "همیشه در ابتدای پاراگراف قرار دارند",
        "ممکن است در نقطه‌ای نسبتاً دور در همان جمله یا پاراگراف حضور داشته باشند"
      ],
      "answer": 3
    },
    {
      "id": "Q019",
      "question": "برای ساخت نمایش در لایهٔ بعدی (k+1)، وزن‌ها و نمایش‌ها از کدام لایه گرفته می‌شوند؟",
      "options": [
        "نمایش‌های محاسبه‌شده در لایهٔ k",
        "نمایش‌های لایهٔ نهایی مدل",
        "فقط نمایش‌های ورودی اولیه قبل از لایه‌ها",
        "نمایش‌های لایهٔ k+1 که هنوز محاسبه نشده‌اند"
      ],
      "answer": 0
    },
    {
      "id": "Q020",
      "question": "مزیت اصلی ساخت نمایش‌های زمینه‌ای برای هر واژه چیست؟",
      "options": [
        "کاهش تعداد توکن‌ها در واژه‌نامهٔ مدل",
        "ادغام معناهای کلمات زمینه‌ای برای فهم بهتر معنای واژه در آن بستر",
        "افزایش سرعت اجرای سخت‌افزار",
        "تحمیل توافق عددی بین فاعل و فعل"
      ],
      "answer": 1
    },
    {
      "id": "Q021",
      "question": "وظیفه محاسبه توجه در یک لایه ترنسفورمر چیست؟",
      "options": [
        "ساخت نمای برداری از یک توکن در آن لایه",
        "پیش‌بینی احتمال دقیق کلمه بعدی",
        "محاسبه مقدار تابع خطا برای آن لایه",
        "تولید یک بردار یکتا برای کل رشته ورودی"
      ],
      "answer": 0
    },
    {
      "id": "Q022",
      "question": "برای تولید خروجی در یک موقعیت معین، مکانیزم توجه چه ورودی‌هایی را می‌گیرد؟",
      "options": [
        "فقط نمایش توکن فعلی در آن موقعیت",
        "فقط نمایش همه توکن‌های قبلی بدون نمایش توکن فعلی",
        "نمایش توکن فعلی همراه با نمایش‌های توکن‌های قبلی در پنجره زمینه",
        "فقط نمایش توکن‌های آینده"
      ],
      "answer": 2
    },
    {
      "id": "Q023",
      "question": "در مدل‌های علّی که از چپ به راست کار می‌کنند، پنجره زمینه عمدتاً شامل چه محتواهایی است؟",
      "options": [
        "تمام توکن‌ها از جمله توکن‌های آینده",
        "فقط کلمات یا توکن‌های قبلی",
        "تنها توکن فعلی",
        "فقط توکن‌هایی که بعد از موقعیت فعلی قرار دارند"
      ],
      "answer": 1
    },
    {
      "id": "Q024",
      "question": "چه محدودیتی دربارهٔ دسترسی مدل هنگام پردازش یک موقعیت فعلی وجود دارد؟",
      "options": [
        "مدل تنها به نمایش همان موقعیت دسترسی دارد",
        "مدل می‌تواند به تمام توکن‌های آینده نگاه کند",
        "مدل فقط به اولین توکن جمله دسترسی دارد",
        "مدل به نمایش توکن‌های قبلی دسترسی دارد اما نه به توکن‌های بعد از موقعیت فعلی"
      ],
      "answer": 3
    },
    {
      "id": "Q025",
      "question": "یک لایه خودتوجه چگونه دنباله ورودی را به خروجی تبدیل می‌کند؟",
      "options": [
        "تولید دنباله‌ای خروجی با همان طول دنباله ورودی",
        "فشرده‌سازی همه ورودی‌ها به یک بردار نهایی",
        "حذف بعضی توکن‌ها برای کوتاه کردن دنباله",
        "افزایش طول دنباله با اضافه کردن توکن‌های جدید"
      ],
      "answer": 0
    },
    {
      "id": "Q026",
      "question": "در توضیح ساده‌شده، خروجی توجه در اصل از چه ترکیبی به‌دست می‌آید؟",
      "options": [
        "یک ترکیب غیرخطی پیچیده از نمایه‌ها",
        "انتخاب تنها نمایه‌ای که بیشترین تکرار را دارد",
        "جمعی وزن‌دار از بردارهای زمینه",
        "میانگین ساده و بدون وزن از همه بردارها"
      ],
      "answer": 2
    },
    {
      "id": "Q027",
      "question": "در نسخه ساده‌شده، وزن مربوط به هر بردار زمینه چگونه تعیین می‌شود؟",
      "options": [
        "بر مبنای موقعیت مکانی توکن‌ها در دنباله",
        "متناسب با میزان شباهت بین بردار توکن جاری و بردار توکن قبلی",
        "به‌صورت تصادفی انتخاب می‌شود",
        "بر اساس طول کاراکترهای توکن"
      ],
      "answer": 1
    },
    {
      "id": "Q028",
      "question": "برای محاسبه امتیاز شباهت بین دو نمایش برداری در متن، از چه عملیاتی استفاده می‌شود؟",
      "options": [
        "حاصل‌ضرب داخلی بردارها",
        "محاسبه فاصله زمانی بین توکن‌ها",
        "مرتب‌سازی بردارها بر اساس اندازه مطلق",
        "ساخت گراف اتصالات بین توکن‌ها"
      ],
      "answer": 0
    },
    {
      "id": "Q029",
      "question": "پس از به‌دست آوردن امتیازهای شباهت خام، چه گامی برای تولید وزن‌های نهایی برداشته می‌شود؟",
      "options": [
        "حذف امتیازهای منفی و نگه داشتن بقیه",
        "تقسیم امتیازها بر طول جمله",
        "انتخاب تنها بهترین امتیاز و صفر کردن سایر امتیازها",
        "نرمال‌سازی امتیازها با استفاده از تابع سافت‌مکس"
      ],
      "answer": 3
    },
    {
      "id": "Q030",
      "question": "برای محاسبه خروجی در موقعیت سوم در نمونه ساده‌شده، کدام فرایند مطابق توضیحات است؟",
      "options": [
        "گرفتن میانگین برابر از نمایش‌های سه موقعیت اول تا سوم بدون توجه به شباهت",
        "محاسبه امتیاز شباهت بین نمایش موقعیت سوم و هر یک از نمایش‌های قبلی و خودش، نرمال‌سازی این امتیازها و سپس جمع وزن‌دار نمایش‌ها",
        "استفاده تنها از نمایش مربوط به موقعیت سوم",
        "نگاه کردن به نمایش توکن‌های پس از موقعیت سوم برای تعیین وزن‌ها"
      ],
      "answer": 1
    },
    {
      "id": "Q031",
      "question": "چرا وزن نرم‌ماکس معمولاً برای بردار یک موقعیت بیشترین مقدار را دارد؟",
      "options": [
        "چون آن بردار معمولاً شبیه‌ترین بردار به خودش است و بنابراین شباهت بالایی دارد",
        "چون مدل به طور صریح یک بایاس برای بردار جاری می‌آموزد",
        "چون بردار جاری همیشه از جلوترین موقعیت استفاده می‌کند",
        "چون نرم‌ماکس مکان بردارها را تغییر می‌دهد"
      ],
      "answer": 0
    },
    {
      "id": "Q032",
      "question": "در فرایند توجه، هر بردار ورودی چه سه نقش متمایزی می‌تواند داشته باشد؟",
      "options": [
        "نمایش، موقعیت، وزن",
        "پیش‌بینی، بازخورد، خطا",
        "پرسش، کلید، مقدار",
        "ورودی، خروجی، میانگین"
      ],
      "answer": 2
    },
    {
      "id": "Q033",
      "question": "برای بازنمایی مستقل نقش‌های پرسش، کلید و مقدار از چه روشی استفاده می‌شود؟",
      "options": [
        "هر بردار ورودی با ماتریس‌های وزن جداگانه به نمایش‌های مربوط به هر نقش نگاشت می‌یابد",
        "از همان بردار ورودی برای هر نقش بدون تغییر استفاده می‌شود",
        "نقش‌ها با اضافه‌کردن نویز تصادفی به بردار اصلی جدا می‌شوند",
        "با استفاده از فیلترهای کانولوشنی محتوای نقش‌ها استخراج می‌شود"
      ],
      "answer": 0
    },
    {
      "id": "Q034",
      "question": "چگونه شباهت بین عنصر جاری و یک عنصر قبلی معمولاً محاسبه می‌شود؟",
      "options": [
        "با اندازه‌گیری فاصله هندسی بدون تبدیل بردارها",
        "با محاسبه اختلاف در مقادیر شاخص موقعیت",
        "با مقایسه ترتیب قرارگیری در دنباله",
        "با ضرب داخلی بین نمایش پرسش عنصر جاری و نمایش کلید عنصر قبلی"
      ],
      "answer": 3
    },
    {
      "id": "Q035",
      "question": "علت مقیاس‌بندی مقدار حاصل از ضرب داخلی پیش از اعمال نرم‌ماکس چیست؟",
      "options": [
        "برای جلوگیری از تولید مقادیر خیلی بزرگ که می‌تواند باعث ناپایداری عددی و از بین رفتن گرادیان‌ها شود",
        "برای تضمین اینکه خروجی همیشه بین صفر و یک باشد",
        "برای اجباری کردن توزیع یکنواخت وزن‌ها",
        "برای حذف مقادیر منفی قبل از نرم‌ماکس"
      ],
      "answer": 0
    },
    {
      "id": "Q036",
      "question": "عملکرد نرم‌ماکس در محاسبه توجه چه چیزی را تولید می‌کند؟",
      "options": [
        "یک بردار خروجی جدید برای عنصر جاری",
        "مجموع مقادیر وزن‌دار عناصر قبلی",
        "یک توزیع احتمال که به عنوان وزن‌ها برای جمع مقادیر استفاده می‌شود",
        "فهرستی از اندیس‌های بیشترین شباهت"
      ],
      "answer": 2
    },
    {
      "id": "Q037",
      "question": "چطور بردار خروجی خودتوجه برای یک عنصر تولید می‌شود؟",
      "options": [
        "با میانگین ساده تمام بردارهای ورودی",
        "با جمع مقادیر عناصر قبلی که هر کدام به وسیله وزن شباهت‌شان ضرب شده‌اند و سپس پردازش خروجی",
        "با انتخاب برداری که بیشترین شباهت را دارد و بازگرداندن همان بردار",
        "با الحاق بردارهای پرسش و کلید"
      ],
      "answer": 1
    },
    {
      "id": "Q038",
      "question": "چه مزیتی از عنوان کردن یک لایه به صورت «هد» در توجه حاصل می‌شود؟",
      "options": [
        "هد باعث کاهش تعداد پارامترها می‌شود",
        "هد به مدل اجازه می‌دهد که فقط به عنصر پیشین مستقیم توجه کند",
        "هد نشان‌دهنده استفاده از فعل و انفعالات کانولوشنی است",
        "هد امکان بازنمایی مجزا و سازمان‌یافته نقش‌های مختلف هر ورودی را فراهم می‌آورد"
      ],
      "answer": 3
    },
    {
      "id": "Q039",
      "question": "در نسخه خودتوجهی که در قطعه متن آمده، هنگام محاسبه توجه برای موقعیت i از کدام عناصر استفاده می‌شود؟",
      "options": [
        "فقط عناصر تا و شامل عنصر جاری (عناصر قبلی و خودِ i)",
        "تمامی عناصر در کل دنباله بدون توجه به موقعیت",
        "تنها عنصر قبلی فوری",
        "فقط عناصر بعدی در دنباله"
      ],
      "answer": 0
    },
    {
      "id": "Q040",
      "question": "چرا به جای استفاده مستقیم از بردار ورودی برای همه نقش‌ها، از نگاشت جداگانه برای هر نقش استفاده می‌کنند؟",
      "options": [
        "برای کاهش پیچیدگی محاسباتی مدل",
        "برای حفظ تعداد پارامترها ثابت",
        "زیرا یک بردار واحد نمی‌تواند هم‌زمان نمایش‌های مناسب برای مقایسه و بازیابی اطلاعات را ارائه دهد",
        "برای جلوگیری از هم‌پوشانی موقعیت‌های زمانی"
      ],
      "answer": 2
    },
    {
      "id": "Q041",
      "question": "در معماری توجه، افزودن ماتریس WO چه کاربردی دارد؟",
      "options": [
        "تغییر شکل خروجی سر توجه به گونه‌ای که با بعد مدل همسان شود",
        "محاسبه مستقیم وزن‌های توجه بین پرسش و کلیدها",
        "ایجاد بردارهای پرسش، کلید و مقدار از ورودی",
        "کاهش نویز اطلاعاتی قبل از اعمال چندسر توجه"
      ],
      "answer": 0
    },
    {
      "id": "Q042",
      "question": "یکی از دلایل نگه داشتن همان بعد برای ورودی و خروجی توجه در ترنسفورمر چیست؟",
      "options": [
        "برای کمینه‌سازی حافظه مصرفی مدل",
        "افزایش ماژولار بودن و سازگاری اجزای مدل",
        "افزایش سرعت محاسبات ضرب درونی بین بردارها",
        "اجتناب از نیاز به ماتریس‌های تبدیل میانی"
      ],
      "answer": 1
    },
    {
      "id": "Q043",
      "question": "بردار پرسش و بردار کلید دارای چه خصوصیتی هستند که اجازه می‌دهد ضرب درونی آنها یک مقدار اسکالر تولید کند؟",
      "options": [
        "هر یک دارای بعد dv هستند و ضرب عنصری تولید می‌کنند",
        "یکی بردار داده و دیگری ماتریس وزن است که با هم ضرب می‌شوند",
        "هر دو بردار دارای همان بعدی به نام dk هستند و ضرب درونی آنها اسکالر می‌سازد",
        "بردار پرسش طولانی‌تر است و مقیاس‌پذیری را افزایش می‌دهد"
      ],
      "answer": 2
    },
    {
      "id": "Q044",
      "question": "قبل از اعمال ماتریس WO، خروجی یک سر توجه چه بعد مفهومی‌ای دارد؟",
      "options": [
        "همان بعد کلی مدل d",
        "بعد برابر با تعداد سرها",
        "بعدی برابر با طول ورودی",
        "بعدی که با dv مشخص می‌شود"
      ],
      "answer": 3
    },
    {
      "id": "Q045",
      "question": "در مثال ذکرشده از کار اولیه ترنسفورمر، چه ترکیب عددی برای ابعاد d، dk و dv آورده شده بود؟",
      "options": [
        "d برابر 256 و dk و dv هر کدام 32",
        "d برابر 512 و dk و dv هر دو 64",
        "d برابر 1024 و dk برابر 128 و dv برابر 64",
        "d برابر 512 و dk برابر 512 و dv برابر 512"
      ],
      "answer": 1
    },
    {
      "id": "Q046",
      "question": "هدف اصلی استفاده از چند سر توجه موازی در یک لایه چیست؟",
      "options": [
        "هر سر بتواند به جنبه‌ها یا الگوهای متفاوتی از زمینه توجه کند",
        "کاهش کلی تعداد پارامترهای مدل",
        "افزایش سرعت اجرا با کاهش محاسبات داخل هر سر",
        "جلوگیری از هم‌پوشانی اطلاعات بین توکن‌ها"
      ],
      "answer": 0
    },
    {
      "id": "Q047",
      "question": "ماتریس‌های WQ، WK و WV چه نقشی در فرایند توجه ایفا می‌کنند؟",
      "options": [
        "آنها وزن‌های خروجی نهایی را ترکیب می‌کنند",
        "آنها برای محاسبه ماتریس WO به کار می‌روند",
        "آنها ورودی را به بردارهای پرسش، کلید و مقدار تبدیل می‌کنند",
        "آنها طول توالی را تغییر می‌دهند تا همگام با سرها شوند"
      ],
      "answer": 2
    },
    {
      "id": "Q048",
      "question": "علاوه بر ورودی و خروجی توجه، کدام اجزا نیز در متن اشاره شده‌اند که همان بعد مدل را دارند؟",
      "options": [
        "فقط وزن‌های ماتریسی در لایه توجه",
        "خروجی هر بلوک ترنسفورمر و بردارهای میانی داخل بلاک",
        "تنها بردارهای مقدار در هر سر توجه",
        "تنها سرهای توجه منفرد"
      ],
      "answer": 1
    },
    {
      "id": "Q049",
      "question": "وقتی گفته می‌شود چند سر توجه جداگانه وجود دارد که هر کدام پارامترهای خود را دارند، منظور چیست؟",
      "options": [
        "هر سر توجه یک نسخه فشرده از کل مدل را دارد",
        "سرها پارامترهای خود را به صورت اشتراکی نگه می‌دارند",
        "هر سر به طور مستقل پارامترهایی دارد تا جنبه‌های متفاوتی از روابط ورودی را مدلسازی کند",
        "تنها یک سر پارامتر مستقل دارد و بقیه بردارهای آن را کپی می‌کنند"
      ],
      "answer": 2
    },
    {
      "id": "Q050",
      "question": "سرهای توجه موازی که در متن ذکر شده‌اند در چه سطحی از مدل قرار می‌گیرند؟",
      "options": [
        "در لایه ابتدایی مدل تنها",
        "در لایه پایانی مدل تنها",
        "در عمق‌های متفاوت و پراکنده در سراسر مدل",
        "در همان عمق و به‌صورت موازی در آن نقطه قرار می‌گیرند"
      ],
      "answer": 3
    },
    {
      "id": "Q051",
      "question": "در یک لایه خودتوجه چندسری، هر سر معمولاً چه مجموعه‌ای از ماتریس‌های وزنی دارد؟",
      "options": [
        "فقط یک ماتریس خروجی مشترک بین همه سرها",
        "ماتریس‌های مخصوص فقط برای پرسش و خروجی",
        "مجموعه‌ای از ماتریس‌های پرسش، کلید و مقدار مخصوص همان سر",
        "ماتریس‌های مربوط به لایه نرمال‌سازی"
      ],
      "answer": 2
    },
    {
      "id": "Q052",
      "question": "هدف از استفاده از ماتریس‌های پرسش، کلید و مقدار برای هر سر چیست؟",
      "options": [
        "پروژه‌کردن ورودی‌ها به بردارهای پرسش، کلید و مقدار مخصوص آن سر",
        "جمع کردن خروجی همه سرها بدون تبدیل",
        "محاسبه‌ی وزن‌های به‌روزرسانی پارامترها",
        "اعمال نرمال‌سازی روی بردارها"
      ],
      "answer": 0
    },
    {
      "id": "Q053",
      "question": "هنگامی که از چند سر استفاده می‌شود، کدام بعد برای بردار ورودی و خروجی مدل حفظ می‌شود؟",
      "options": [
        "بعد بردار پرسش و کلید",
        "بعد مدل که نماینده طول بردار ورودی و خروجی است",
        "بعد مقدار",
        "تعداد سرها"
      ],
      "answer": 1
    },
    {
      "id": "Q054",
      "question": "در مقاله‌ی اصلی ترنسفورمر چه مقادیری برای ابعاد dk و dv، تعداد سرها و بعد مدل ذکر شده‌اند؟",
      "options": [
        "dk و dv برابر 128، تعداد سرها 4، بعد مدل 256",
        "dk و dv برابر 32، تعداد سرها 16، بعد مدل 1024",
        "dk برابر 64 و dv برابر 128، تعداد سرها 8، بعد مدل 512",
        "dk و dv هر دو برابر 64، تعداد سرها 8، بعد مدل 512"
      ],
      "answer": 3
    },
    {
      "id": "Q055",
      "question": "در حالت توجه علی یا چپ‌به‌راست که در این فصل استفاده می‌شود، «سایر ورودی‌ها» برای هر موقعیت کدام توکن‌ها هستند؟",
      "options": [
        "فقط توکن‌هایی که در موقعیت‌های قبلی (چپ) قرار دارند",
        "تنها توکن‌های بعدی (به راست)",
        "تمام توکن‌های جمله بدون توجه به ترتیب",
        "فقط توکن‌های غیرمجاور"
      ],
      "answer": 0
    },
    {
      "id": "Q056",
      "question": "پس از اینکه خروجی هر سر محاسبه شد، چه روندی برای تبدیل آن به خروجی نهایی مدل انجام می‌شود؟",
      "options": [
        "هر خروجی سر به طور مستقل به بعد مدل نگاشت زده می‌شود و سپس جمع می‌شود",
        "فقط خروجی سر اول به عنوان خروجی نهایی انتخاب می‌شود",
        "خروجی‌های همه سرها کنار هم قرار می‌گیرند و سپس با یک نگاشت خطی به بعد مدل بازنگاشت می‌شوند",
        "خروجی‌ها فشرده شده و به صورت میانگین گرفته می‌شوند"
      ],
      "answer": 2
    },
    {
      "id": "Q057",
      "question": "بلاک ترنسفورمر علاوه بر لایه خودتوجه شامل چه اجزایی است؟",
      "options": [
        "فقط لایه‌های کانولوشن و دراپ‌اوت",
        "یک لایه فیدفوروارد، اتصال‌های باقیمانده و لایه‌های نرمال‌سازی",
        "چندین لایه بازگشتی همراه با حافظه بلندمدت",
        "تنها یک لایه خطی اضافی"
      ],
      "answer": 1
    },
    {
      "id": "Q058",
      "question": "دیدگاه جریان باقیمانده درباره پردازش یک توکن چه تصویری ارائه می‌دهد؟",
      "options": [
        "پردازش توکن‌ها به صورت جداگانه و بدون اشتراک اطلاعات",
        "یک سلسله مراتب چندجمله‌ای از لایه‌ها که اطلاعات را فشرده می‌کند",
        "فقط مسیر مستقیم از ورودی به خروجی بدون افزوده شدن اطلاعات",
        "یک جریان واحد از نمایش‌های با بعد مدل که اجزا خروجی‌شان را می‌خوانند و به آن اضافه می‌کنند"
      ],
      "answer": 3
    },
    {
      "id": "Q059",
      "question": "ورودی اولیه‌ای که در پایین جریان باقیمانده قرار می‌گیرد معمولاً چیست؟",
      "options": [
        "یک بردار جاسازی برای همان توکن که دارای بعد مدل است",
        "یک بردار صفر برای همه توکن‌ها",
        "نمایش میانگین تمام توکن‌های جمله",
        "یک بردار تصادفی که در طول آموزش تغییر نمی‌کند"
      ],
      "answer": 0
    },
    {
      "id": "Q060",
      "question": "آیا پرسش‌ها، کلیدها و مقادیر برای همه سرها با همان ماتریس‌ها محاسبه می‌شوند یا هر سر مجموعه‌ای جداگانه دارد؟",
      "options": [
        "همه سرها از یک مجموعه ماتریس مشترک استفاده می‌کنند",
        "فقط کلیدها و مقادیر برای هر سر جدا هستند، پرسش‌ها مشترک است",
        "هر سر مجموعه‌ای جداگانه از ماتریس‌های پرسش، کلید و مقدار دارد",
        "فقط ماتریس خروجی برای هر سر مجزا است"
      ],
      "answer": 2
    },
    {
      "id": "Q061",
      "question": "کدام توصیف درباره لایه فیدفوروارد در یک بلوک ترنسفورمر درست است؟",
      "options": [
        "شبکه‌ای کاملاً متصل با دو لایه و یک لایه پنهان است که وزن‌ها بین موقعیت‌های توکنی یکسان اما بین لایه‌ها متفاوتند",
        "وزن‌های آن برای هر موقعیت توکنی متفاوت است و بین لایه‌ها یکسان می‌ماند",
        "ابعاد لایه پنهان همیشه برابر با بعد مدل است",
        "لایه فیدفوروارد همان مکانیزم توجه چندسر است"
      ],
      "answer": 0
    },
    {
      "id": "Q062",
      "question": "نرمال‌سازی لایه (لِیینرمال) در ترنسفورمر روی چه چیزی اعمال می‌شود؟",
      "options": [
        "روی همه توکن‌های موجود در یک ورودی به‌صورت هم‌زمان",
        "روی برداری که نمایانگر یک توکن منفرد است",
        "روی پارامترهای وزن یک لایه",
        "روی خروجی نهایی تمام بلوک‌ها"
      ],
      "answer": 1
    },
    {
      "id": "Q063",
      "question": "هدف اصلی اجرای نرمال‌سازی لایه در شبکه‌های عمیق چیست؟",
      "options": [
        "کاهش تعداد پارامترها در مدل",
        "تبدیل بردارها به بردارهای دارای توزیع گوسی دقیق",
        "قرار دادن مقادیر یک بردار در بازه‌ای که آموزش مبتنی بر گرادیان را تسهیل کند و سپس اعمال ضریب و بایاس قابل یادگیری",
        "جایگزینی مکانیزم توجه برای انتقال اطلاعات بین توکن‌ها"
      ],
      "answer": 2
    },
    {
      "id": "Q064",
      "question": "در یک بلوک ترنسفورمر استاندارد، چند بار نرمال‌سازی لایه انجام می‌شود داخل همان بلوک (قبل از نرمال‌سازی نهایی در انتهای شبکه)؟",
      "options": [
        "یک بار",
        "سه بار",
        "چهار بار",
        "دو بار"
      ],
      "answer": 3
    },
    {
      "id": "Q065",
      "question": "کدام جزءِ بلوک ترنسفورمر اطلاعات را از توکن‌های دیگر دریافت می‌کند؟",
      "options": [
        "مکانیزم توجه چندسر",
        "شبکه فیدفوروارد دو لایه",
        "نرمال‌سازی لایه",
        "جمع با مسیر رزیدوال"
      ],
      "answer": 0
    },
    {
      "id": "Q066",
      "question": "طبق توضیحات، جریان رزیدوال در لایه‌های ابتدایی و در لایه‌های بالاتر معمولاً چه نمایشی از توکن‌ها دارد؟",
      "options": [
        "در همه لایه‌ها نمایانگر توکن بعدی است",
        "در لایه‌های ابتدایی نمایانگر توکن جاری و در لایه‌های بالاتر معمولاً نمایانگر توکن بعدی است",
        "همیشه فقط اطلاعات همسایه‌ها را نگه می‌دارد",
        "در لایه‌های بالاتر هیچ نمایشی از توکن‌ها وجود ندارد"
      ],
      "answer": 1
    },
    {
      "id": "Q067",
      "question": "چرا خروجی و ورودی هر بلوک ترنسفورمر دارای همان بعد برداری هستند؟",
      "options": [
        "برای کاهش محاسبات ماتریسی",
        "برای جدا کردن فضای تعبیه‌بندی به زیربخش‌های مستقل",
        "تا امکان قرار دادن چند بلوک پشت سر هم و ساختن مدل‌های چندلایه فراهم شود",
        "به‌خاطر محدودیت‌های توجه چندسر"
      ],
      "answer": 2
    },
    {
      "id": "Q068",
      "question": "چطور می‌توان نقش یک سر توجه را در انتقال اطلاعات بین توکن‌ها توصیف کرد؟",
      "options": [
        "سر توجه فقط وزن‌های نرمال‌سازی را محاسبه می‌کند",
        "سر توجه بردار توکن فعلی را حذف می‌کند و جایگزین می‌سازد",
        "سر توجه اطلاعات را به‌صورت کامل به توکن مقصد کپی می‌کند",
        "سر توجه می‌تواند اطلاعات را از جریان رزیدوال یک توکن همسایه به جریان توکن جاری منتقل کند"
      ],
      "answer": 3
    },
    {
      "id": "Q069",
      "question": "در ترتیب عملیات یک بلوک ترنسفورمر، لایه فیدفوروارد بعد از کدام مرحله اعمال می‌شود؟",
      "options": [
        "پس از نرمال‌سازی لایه دوم که به دنبال جمع رزیدوال آمده است",
        "بلافاصله پس از لایه نرمال‌سازی اول",
        "قبل از اعمال مکانیزم توجه",
        "پس از اجرای نرمال‌سازی نهایی در انتهای شبکه"
      ],
      "answer": 0
    },
    {
      "id": "Q070",
      "question": "در عمل، مدل‌های بزرگ زبانی معمولاً چند بلوک ترنسفورمر را روی هم قرار می‌دهند؟",
      "options": [
        "همیشه تنها یک بلوک",
        "از حدود دوازده بلوک تا ده‌ها بلوک بسته به اندازه مدل",
        "به‌طور ثابت دوازده بلوک برای تمام مدل‌ها",
        "همیشه بیش از هزار بلوک"
      ],
      "answer": 1
    },
    {
      "id": "Q071",
      "question": "هدف از قرار دادن همهٔ بردارهای ورودی در یک ماتریس X چیست؟",
      "options": [
        "برای کاهش تعداد لایه‌های ترنسفورمر",
        "برای ذخیرهٔ کمتر حافظه در هنگام آموزش",
        "برای امکان پردازش موازی و بهره‌گیری از ضرب ماتریسی کارا",
        "برای افزایش طول بردارهای جاسازی"
      ],
      "answer": 2
    },
    {
      "id": "Q072",
      "question": "چرا محاسبهٔ توجه برای هر توکن را می‌توان مستقل از بقیه در نظر گرفت؟",
      "options": [
        "زیرا محاسبات برای هر توکن وابسته به توکن بعدی است",
        "زیرا عملیات لازم برای هر توکن جداگانه بوده و قابل موازی‌سازی است",
        "زیرا فقط در سرِ آخر مدل مستقل است",
        "زیرا فقط در مدل‌های چندسره این استقلال برقرار است"
      ],
      "answer": 1
    },
    {
      "id": "Q073",
      "question": "برای ترنسفورمرهای معمولی طول ورودی N معمولاً در چه بازه‌ای قرار دارد؟",
      "options": [
        "بین ده تا صد",
        "همیشه کمتر از هزار",
        "دقیقاً برابر با یک میلیون",
        "معمولاً از حدود هزار تا سی‌و‌دو هزار"
      ],
      "answer": 3
    },
    {
      "id": "Q074",
      "question": "وقتی ماتریس X را با ماتریس‌های مربوط به کوئری، کلید و مقدار ضرب می‌کنیم، هر یک از ماتریس‌های حاصل چه چیزی در بردارند؟",
      "options": [
        "مجموعهٔ بردارهای کوئری، کلید یا مقدار برای تمام توکن‌ها",
        "بردارهای خروجی نهایی مدل برای هر توکن",
        "وزن‌های به‌روز‌رسانی گرادیان‌ها",
        "بردارهای موقعیت زمانی توکن‌ها"
      ],
      "answer": 0
    },
    {
      "id": "Q075",
      "question": "ضرب ماتریسی بین ماتریس‌های کوئری و ترانهادهٔ کلید چه اطلاعاتی را به‌دست می‌دهد؟",
      "options": [
        "مجموع نهایی توجه پس از نرم‌سازی",
        "نتیجهٔ ضربِ مقادیری که جایگزین بردارهای مقدار می‌شوند",
        "ماتریسی از مقایسه‌های جفتی بین هر کوئری و هر کلید برای تمام توکن‌ها",
        "نمایش ترتیبی از موقعیت‌های توکن‌ها"
      ],
      "answer": 2
    },
    {
      "id": "Q076",
      "question": "برای رسیدن به زمینه‌های خیلی طولانی‌تر از مقادیر معمول مثل صدها هزار توکن، چه کاری لازم است؟",
      "options": [
        "تنها افزایش بعد جاسازی کافی است",
        "استفاده از یک سر توجه منفرد",
        "پاک کردن ماتریس X بین هر لایه",
        "اعمال تغییرات معماری مانند مکانیزم‌های مخصوص زمینهٔ طولانی"
      ],
      "answer": 3
    },
    {
      "id": "Q077",
      "question": "پس از محاسبهٔ ماتریس مقایسهٔ کوئری‌ها و کلیدها چه گام‌هایی پیش می‌آید تا بردار نمایش هر توکن تولید شود؟",
      "options": [
        "مقیاس‌گذاری امتیازها، اعمال نرم‌افزار نرم‌سازی، و سپس ضرب در ماتریس مقدار",
        "مستقیماً استفاده از ماتریس مقایسه به‌عنوان خروجی نهایی",
        "جمع‌کردن کوئری و کلید و سپس ارسال به لایهٔ بازخور",
        "اول ضرب در مقدار و سپس اعمال نرم‌افزار نرم‌سازی"
      ],
      "answer": 0
    },
    {
      "id": "Q078",
      "question": "در توضیح نویسنده، ترتیب بررسی توجه به چه صورت است؟",
      "options": [
        "ابتدا شرح چند سر، سپس تبدیل به تک‌سر",
        "ابتدا یک سر توجه را بررسی کرده و بعد سرهای متعدد را مطرح می‌کنند",
        "فقط حالت چند سر توضیح داده می‌شود و تک‌سر بررسی نمی‌شود",
        "مستقیماً کل بلوک ترنسفورمر بدون اشاره به سرها شرح داده می‌شود"
      ],
      "answer": 1
    },
    {
      "id": "Q079",
      "question": "در ماتریس X هر سطر نشان‌دهندهٔ چه چیزی است؟",
      "options": [
        "بردار جاسازی یک توکن از ورودی",
        "وزن‌های توجه بین دو توکن",
        "مجموع کلیدها و کوئری‌ها",
        "نتیجهٔ ضرب QK ترانهاده"
      ],
      "answer": 0
    },
    {
      "id": "Q080",
      "question": "کاهش کل مرحلهٔ خودتوجه برای دنباله‌ای از توکن‌ها به ضرب ماتریسی چه مزیتی فراهم می‌آورد؟",
      "options": [
        "حذف نیاز به لایه‌های باقیمانده",
        "امکان محاسبهٔ هم‌زمان و کارا برای تمام توکن‌ها",
        "جایگزینی نرم‌افزار نرم‌سازی با عملیات خطی ساده",
        "کاهش تعداد توکن‌های ورودی مورد نیاز"
      ],
      "answer": 1
    },
    {
      "id": "Q081",
      "question": "هدف اصلی استفاده از ماسک در محاسبهٔ توجهِ خودی (self-attention) چیست؟",
      "options": [
        "جلوگیری از دسترسی مدل به توکن‌هایی که بعد از توکن فعلی می‌آیند",
        "افزایش سرعت ضرب ماتریسی در لایه توجه",
        "کاهش تعداد سرهای چندسرّی برای سبک‌تر کردن مدل",
        "به هم‌زدن ترتیب توکن‌ها برای تقویت یادگیری"
      ],
      "answer": 0
    },
    {
      "id": "Q082",
      "question": "برای بی‌اثر کردن تأثیر توکن‌های بعدی در مقایسه‌های پرسش-کلید چگونه عمل می‌شود؟",
      "options": [
        "جایگزینی آن‌ها با صفر معمولی",
        "تنظیم عناصر بالاتِر مثلثی ماتریس به مقدار منفی بی‌نهایت تا نرم‌افزار نرم‌افزاری آن‌ها را صفر کند",
        "افزودن نویز تصادفی تا وزن‌ها ناپایدار شوند",
        "ضرب کردن آن بخش از ماتریس در یک ضریب کوچک"
      ],
      "answer": 1
    },
    {
      "id": "Q083",
      "question": "چرا در مسئلهٔ مدل‌سازی زبان قرار دادن اطلاعات آینده در محاسبهٔ توجه مشکل‌ساز است؟",
      "options": [
        "چون باعث افزایش اندازهٔ ماتریس وزن‌ها می‌شود",
        "زیرا استفاده از اطلاعات آینده منجر به ناپایداری عددی می‌شود",
        "زیرا اگر مدل بتواند به کلمات آینده دسترسی داشته باشد، حدس کلمهٔ بعدی به‌شدت ساده و تقلب‌آمیز خواهد شد",
        "به خاطر آنکه اثرات متناظر را در لایه‌های بالاتر حذف می‌کند"
      ],
      "answer": 2
    },
    {
      "id": "Q084",
      "question": "پیچیدگی محاسباتی توجه نسبت به طول توالی ورودی چگونه افزایش می‌یابد؟",
      "options": [
        "بصورت خطی با طول توالی افزایش می‌یابد",
        "به صورت ثابت مستقل از طول توالی است",
        "به صورت لگاریتمی با طول توالی رشد می‌کند",
        "به صورت مربعی نسبت به طول توالی افزایش پیدا می‌کند"
      ],
      "answer": 3
    },
    {
      "id": "Q085",
      "question": "در مکانیزم چندسرّی، خروجی‌های سرها چگونه برای ادامهٔ پردازش ترکیب می‌شوند؟",
      "options": [
        "جمع برداری سرها و سپس اعمال یک لایهٔ غیرخطی",
        "میانگین‌گیری از خروجی سرها و ارسال به لایهٔ بعدی",
        "به‌هم‌چسباندن ماتریس‌های خروجی همهٔ سرها و سپس اعمال یک نگاشت خطی نهایی",
        "ضرب المان‌به‌المان خروجی سرها برای فشرده‌سازی"
      ],
      "answer": 2
    },
    {
      "id": "Q086",
      "question": "کدام گزاره درست دربارهٔ ابعاد در توجه چندسرّی است؟",
      "options": [
        "ابعاد ورودی و خروجی یک لایهٔ توجه برابر بعد کلی مدل است، در حالی که بردارهای کلید، پرسش و مقدار ابعادی مستقل دارند",
        "ابعاد ورودی همیشه بزرگتر از بعد مدل است",
        "ابعاد کلید و پرسش با بعد خروجی برابر هستند و همه یک بعد دارند",
        "ابعاد مقدار همواره صفر است تا صرفه‌جویی حافظه شود"
      ],
      "answer": 0
    },
    {
      "id": "Q087",
      "question": "خروجی هر سر در یک لایهٔ چندسرّی چطور سازمان‌دهی می‌شود نسبت به تعداد توکن‌ها و بعد بردار مقدار؟",
      "options": [
        "یک بردار منفرد مستقل از تعداد توکن‌ها",
        "یک ماتریس که ردیف‌های آن مطابق تعداد توکن‌ها و ستون‌های آن مطابق بعد بردار مقدار است",
        "یک اسکالر برای هر سر که خلاصه‌ای از توکن‌هاست",
        "یک لیست بدون ساختار از بردارهای با اندازهٔ برابر بعد مدل"
      ],
      "answer": 1
    },
    {
      "id": "Q088",
      "question": "نقش لایهٔ خطی نهایی که روی ترکیب خروجی سرها اعمال می‌شود چیست؟",
      "options": [
        "اعمال ماسک روی خروجی برای حذف اطلاعات آینده",
        "انجام نرمال‌سازی لایه‌ای قبل از جمع با ورودی اصلی",
        "افزایش طول توالی برای لایه‌های بعدی",
        "بازآرایی و تبدیل برداری خروجیِ چسبیده به بُعد اصلی مدل برای هر توکن"
      ],
      "answer": 3
    },
    {
      "id": "Q089",
      "question": "معادلاتی که با افزودن خروجی توجه چندسرّی به ورودی و سپس اعمال شبکهٔ تغذیهٔ تماماً متصل نشان داده می‌شوند چه ساختاری را نمایش می‌دهند؟",
      "options": [
        "استفاده از اتصال باقیمانده (رزیدو) و اعمال نرمال‌سازی لایه قبل از هر زیرلایه",
        "عدم استفاده از هرگونه اتصال بین لایه‌ها",
        "اینکه نرمال‌سازی همیشه بعد از جمع انجام می‌شود",
        "استفاده از فقط یک زیرلایهٔ توجه بدون شبکهٔ تغذیه‌ای"
      ],
      "answer": 0
    },
    {
      "id": "Q090",
      "question": "چرا محاسبهٔ توجه روی اسناد بسیار طولانی هزینه‌بر است؟",
      "options": [
        "چون اندازهٔ بردارهای توکن‌ها با طول سند افزایش می‌یابد",
        "به‌خاطر محدودیت‌های موازی‌سازی سخت‌افزاری که اجازهٔ پردازش همزمان را نمی‌دهد",
        "زیرا باید ضرب‌های اسکالر بین هر جفت توکن انجام شود که باعث رشد رفتاری هزینه با تعداد جفت‌ها می‌شود",
        "چون ماسک اعمال‌شده برای حذف آینده باعث محاسبات اضافی زیادی می‌شود"
      ],
      "answer": 2
    },
    {
      "id": "Q091",
      "question": "ترتیب درست عملیات در یک بلوک ترانسفورمر از ورودی تا خروجی کدام است؟",
      "options": [
        "ابتدا نرمال‌سازی لایه روی ورودی، سپس توجه چندسر، جمع با ورودی، دوباره نرمال‌سازی، شبکه پیش‌خور و در نهایت جمع با حالت میانی",
        "ابتدا توجه چندسر، سپس نرمال‌سازی، بعد شبکه پیش‌خور و در پایان جمع با ورودی",
        "ابتدا شبکه پیش‌خور، سپس نرمال‌سازی، بعد توجه چندسر و در نهایت جمع با حالت میانی",
        "ابتدا نرمال‌سازی، شبکه پیش‌خور، توجه چندسر و در آخر جمع با ورودی"
      ],
      "answer": 0
    },
    {
      "id": "Q092",
      "question": "وقتی گفته می‌شود همان شبکه پیش‌خور به‌طور موازی روی هر یک از بردارهای پنجره اعمال می‌شود، منظور چیست؟",
      "options": [
        "یک نسخه متفاوت از شبکه برای هر موقعیت توکن استفاده می‌شود",
        "همان پارامترهای شبکه به‌صورت جداگانه روی هر بردار موقعیت اعمال می‌شود",
        "شبکه پیش‌خور فقط روی بردار میانگین تمام توکن‌ها اجرا می‌شود",
        "شبکه پیش‌خور تنها روی اولین توکن پنجره اجرا می‌شود"
      ],
      "answer": 1
    },
    {
      "id": "Q093",
      "question": "کدام گزاره درباره نرمال‌سازی درون بلوک ترانسفورمر صحیح است؟",
      "options": [
        "نرمال‌سازی فقط روی بردار خروجی نهایی انجام می‌شود",
        "نرمال‌سازی مقادیر توجه را از طریق تمام توکن‌ها جمع می‌کند",
        "نرمال‌سازی جایگزین جمع باقیمانده می‌شود",
        "هر توکن به‌طور مستقل و موازی در لایه نرمال‌سازی پردازش می‌شود"
      ],
      "answer": 3
    },
    {
      "id": "Q094",
      "question": "چرا ابعاد ورودی و خروجی بلوک‌های ترانسفورمر یکسان نگه داشته می‌شوند؟",
      "options": [
        "برای کاهش تعداد پارامترها در مدل",
        "برای آسان‌تر کردن محاسبات توجه چندسر",
        "تا بتوان بلوک‌ها را به صورت متوالی روی هم قرار داد و اتصال باقی‌مانده برقرار بماند",
        "برای اینکه بردارهای موقعیت و کلمه نیاز به یک اندازه متفاوت نداشته باشند"
      ],
      "answer": 2
    },
    {
      "id": "Q095",
      "question": "چگونه بردار توکنی برای هر کلمه از ماتریس نگاشت استخراج می‌شود؟",
      "options": [
        "با استفاده از شاخص واژه و انتخاب سطر متناظر در ماتریس نگاشت",
        "با میانگین‌گیری از همه سطرهای ماتریس نگاشت",
        "با ضرب ماتریس موقعیت در ماتریس نگاشت",
        "با اعمال لایه توجه روی تمام سطرها و انتخاب اولین خروجی"
      ],
      "answer": 0
    },
    {
      "id": "Q096",
      "question": "اگر یک بردار یک‌تایی (one-hot) را در ماتریس نگاشت ضرب کنیم، چه نتیجه‌ای حاصل می‌شود؟",
      "options": [
        "جمع همه سطرها به‌دست می‌آید",
        "تبدیل ماتریس به بردار احتمال رخ می‌دهد",
        "ستون‌های ماتریس جابه‌جا می‌شوند",
        "سطر مربوط به آن یک در بردار یک‌تایی استخراج می‌شود"
      ],
      "answer": 3
    },
    {
      "id": "Q097",
      "question": "کدام توصیف مربوط به روش ساده‌ی موقعیت‌یابی مطلق است؟",
      "options": [
        "موقعیت‌ها با قواعد دستوری از روی کلمات استخراج می‌شوند",
        "برای هر موقعیت ورودی یک بردار موقعیت یادگرفتنی و جداگانه وجود دارد که همراه بردار کلمه افزوده می‌شود",
        "موقعیت‌ها به صورت دینامیک از توجه چندسر محاسبه می‌شوند و ذخیره نمی‌شوند",
        "بردار موقعیت فقط پس از آموزش کامل مدل محاسبه و اضافه می‌شود"
      ],
      "answer": 1
    },
    {
      "id": "Q098",
      "question": "پس از جمع بردار کلمه و بردار موقعیت چه چیزی به دست می‌آید؟",
      "options": [
        "یک بردار جدید با بعدی متفاوت که برای توجه چندسر لازم است",
        "یک بردار جدید که طول آن دو برابر بردار کلمه است",
        "یک بردار ورودی جدید با همان طول بردار اصلی که برای مراحل بعدی استفاده می‌شود",
        "فقط بردار موقعیت باقی می‌ماند و بردار کلمه نادیده گرفته می‌شود"
      ],
      "answer": 2
    },
    {
      "id": "Q099",
      "question": "نمایش یک توالی به‌صورت ماتریسی از بردارهای یک‌تایی به چه معناست؟",
      "options": [
        "هر سطر ماتریس نشان‌دهنده یک بردار یک‌تایی برای موقعیت متناظر در پنجره است",
        "هر ستون ماتریس نشان‌دهنده یک بردار توکن است",
        "هر خانه ماتریس احتمال وقوع یک واژه در همه موقعیت‌ها را نشان می‌دهد",
        "این ماتریس همان ماتریس بردارهای موقعیت است"
      ],
      "answer": 0
    },
    {
      "id": "Q100",
      "question": "کدام گزاره درباره ماتریس نگاشت واژگان صحیح است؟",
      "options": [
        "این ماتریس یک سطر برای هر نماد در واژگان دارد و هر سطر بردار نگاشت آن نماد است",
        "هر ستون این ماتریس مربوط به یک نماد واژگان است",
        "این ماتریس بردارهای موقعیت را نگهداری می‌کند نه بردارهای کلمه",
        "در این ماتریس وزن‌های لایه توجه ذخیره می‌شود"
      ],
      "answer": 0
    },
    {
      "id": "Q101",
      "question": "نمایش نهایی هر توکن در ماتریس ورودی چگونه ساخته می‌شود؟",
      "options": [
        "با جمع بردار جاسازی شناسهٔ توکن و بردار جاسازی موقعیت همان مکان",
        "فقط از بردار جاسازی شناسهٔ توکن استفاده می‌شود",
        "فقط از بردار جاسازی موقعیت استفاده می‌شود",
        "بردار نمایش برای هر توکن به‌صورت تصادفی اختصاص داده می‌شود"
      ],
      "answer": 0
    },
    {
      "id": "Q102",
      "question": "کدام یک از مشکلات روش سادهٔ جاسازی موقعیت در فرایند آموزش مدل اشاره شده است؟",
      "options": [
        "در همهٔ موقعیت‌ها دادهٔ آموزشی به یک اندازه موجود است",
        "این روش باعث افزایش قابل توجه پارامترهای مدل می‌شود",
        "برای موقعیت‌های نزدیک ابتدای ورودی دادهٔ آموزشی فراوان و برای موقعیت‌های دورتر کمتر است که ممکن است منجر به آموزش ضعیف آنها و عدم تعمیم خوب شود",
        "بردارهای موقعیت موجب کاهش طول توالی ورودی می‌شوند"
      ],
      "answer": 2
    },
    {
      "id": "Q103",
      "question": "چه نوع جایگزینی برای مقابله با مسئلهٔ طول‌های مختلف توالی‌ها مطرح شده است؟",
      "options": [
        "استفاده از بردارهای موقعیت یادگرفتنی جدا برای هر شاخص",
        "انتخاب یک تابع ایستا که اعداد صحیح را به بردارهای حقیقی نگاشت می‌دهد، مانند ترکیب سینوس و کسینوس با فرکانس‌های مختلف",
        "اختصاص بردارهای تصادفی ثابت به هر موقعیت",
        "حذف هرگونه اطلاعات موقعیت از ورودی"
      ],
      "answer": 1
    },
    {
      "id": "Q104",
      "question": "جاسازی‌های مبتنی بر توابع سینوسی چه ویژگی مهمی را در رابطه با موقعیت‌ها منعکس می‌کنند؟",
      "options": [
        "آنها معنای معنایی کلمات را بهبود می‌بخشند",
        "آنها ترتیب و نزدیکی بین موقعیت‌ها را بهتر نشان می‌دهند، به‌طوری که موقعیت‌های نزدیک‌تر نمایش‌های مشابه‌تری دارند",
        "آنها صرفاً جایگاه‌های دور را تقویت می‌کنند",
        "آنها محاسبات توجه را حذف می‌کنند"
      ],
      "answer": 1
    },
    {
      "id": "Q105",
      "question": "روش‌های پیشرفته‌تر موقعیت‌یابی چه تفاوتی نسبت به افزودن یک‌بارهٔ بردار موقعیت در ابتدای ورودی دارند؟",
      "options": [
        "به‌جای نمایش مطلق، موقعیت نسبی بین توکن‌ها را نشان می‌دهند و معمولاً در مکانیزم توجه در هر لایه اعمال می‌شوند",
        "همهٔ اطلاعات موقعیت را در اولین مرحلهٔ پردازش حذف می‌کنند",
        "فقط برای مدل‌های کوچک کاربرد دارند",
        "بهتر است قبل از هر لایه حذف شوند"
      ],
      "answer": 0
    },
    {
      "id": "Q106",
      "question": "در متن، منظور از عبارت «language modeling head» یا سر مدل زبانی چیست؟",
      "options": [
        "یک تابع هزینهٔ ویژه برای تنظیم وزن‌ها در آموزش",
        "یک مدار یا ماژول اضافه که بر بالای ساختار پایهٔ ترنسفورمر افزوده می‌شود تا مدل‌سازی زبان را انجام دهد",
        "یک نوع لایهٔ توجه اختصاصی برای ترجمه",
        "روشی برای فشرده‌سازی بردارهای خروجی"
      ],
      "answer": 1
    },
    {
      "id": "Q107",
      "question": "وظیفهٔ اصلی مدل‌های زبانی چگونه توصیف شده است؟",
      "options": [
        "فشرده‌سازی متن به بردارهای کوچک برای ذخیره‌سازی",
        "تعیین زبان نوشته‌شده در یک متن",
        "با در نظر گرفتن زمینهٔ قبلی، تخصیص احتمالات مشروط به هر واژهٔ ممکن برای پیش‌بینی واژهٔ بعدی و تولید یک توزیع روی واژگان",
        "تبدیل متن به نمایش‌های تصویری"
      ],
      "answer": 2
    },
    {
      "id": "Q108",
      "question": "در مدل‌های ترنسفورمر، منظور از اندازهٔ بافت (context) چیست؟",
      "options": [
        "تعداد توکن‌هایی است که مدل می‌تواند به‌عنوان زمینه برای پیش‌بینی در نظر بگیرد و این مقدار می‌تواند برای مدل‌های بزرگ بسیار زیاد باشد، مثلاً ده‌ها هزار توکن",
        "تعداد نمونه‌های آموزشی موجود در مجموعهٔ داده",
        "تعداد پارامترهای قابل آموزش در مدل",
        "تعداد لایه‌های شبکه"
      ],
      "answer": 0
    },
    {
      "id": "Q109",
      "question": "سر مدل زبانی چگونه از خروجی لایهٔ آخر برای پیش‌بینی کلمهٔ بعدی استفاده می‌کند؟",
      "options": [
        "خروجی لایهٔ آخر را مستقیماً به متن تبدیل می‌کند بدون هیچ نگاشتی",
        "فقط بردار اولین توکن را برای پیش‌بینی به‌کار می‌گیرد",
        "تمام بردارهای لایهٔ آخر را با هم جمع کرده و سپس حذف می‌کند",
        "بردار متناظر با آخرین توکن در خروجی را می‌گیرد و با استفاده از آن توزیع احتمال روی واژگان برای پیش‌بینی کلمهٔ بعدی تولید می‌کند"
      ],
      "answer": 3
    },
    {
      "id": "Q110",
      "question": "نقش اولین ماژول در فرایند تبدیل خروجی آخرین توکن به توزیع احتمالات واژگانی چیست؟",
      "options": [
        "اعمال یک فیلتر حذف نویز روی بردارها",
        "یک لایهٔ خطی که خروجی بردار آخرین توکن را به صورت نگاشتی مناسب برای تولید احتمالات واژگان تبدیل می‌کند",
        "اعمال یک تابع فعال‌سازی غیرخطی پیچیده برای ساختاردهی بردارها",
        "بازآرایی تصادفی ترتیب بردارهای خروجی"
      ],
      "answer": 1
    }
  ],
  "by_chunk": [
    {
      "chunk_id": "CHUNK_01",
      "questions": [
        {
          "id": "Q001",
          "question": "مکانیسمی که در ترنسفورمر برای ساختن نمایش‌های متنیِ متن-محور یک توکن استفاده می‌شود کدام است؟",
          "options": [
            "توجهِ خودی یا چندسَرِ توجه",
            "پردازش کانولوشنی",
            "نمونه‌برداری تصادفی",
            "شبکهٔ بازگشتی کلاسیک"
          ],
          "answer": 0
        },
        {
          "id": "Q002",
          "question": "در مدل‌سازی زبانیِ چپ‌به‌راست (خودِ علی)، پیش‌بینی توکن‌ها به چه صورت انجام می‌شود؟",
          "options": [
            "همهٔ توکن‌ها به‌صورت هم‌زمان پیش‌بینی می‌شوند",
            "هر توکن یکی‌یکی با شرط‌گذاری روی زمینهٔ قبلی تولید می‌شود",
            "توکن‌ها به ترتیب معکوس پیش‌بینی می‌شوند",
            "بدون استفاده از زمینه، به‌صورت تصادفی انتخاب می‌شوند"
          ],
          "answer": 1
        },
        {
          "id": "Q003",
          "question": "کدام‌یک از موارد زیر ترکیب رایج یک بلوکِ ترنسفورمر را تشکیل می‌دهد؟",
          "options": [
            "لایهٔ توجهِ چندسَر، شبکهٔ خوراک‌رو به جلو و مراحل نرمال‌سازی",
            "لایهٔ کانولوشن، استخرینگ و تبدیل فوریه",
            "شبکهٔ بازگشتی، LSTM و دراپ‌اوت",
            "ماتریس خروجی، تابع هزینه و کرنل"
          ],
          "answer": 0
        },
        {
          "id": "Q004",
          "question": "وظیفهٔ بخشِ رمزنگاری ورودیِ پیش از ستون بلوک‌ها در ترنسفورمر چیست؟",
          "options": [
            "تبدیل یک توکن به یک بردار با استفاده از ماتریسِ جاسازی و رمزگذاری موقعیت",
            "تبدیل بردار خروجی نهایی به توکن‌های کلمه",
            "اعمال نرمال‌سازی روی خروجی‌های ستون بلوک‌ها",
            "ذخیرهٔ تمامی توکن‌ها در یک حافظهٔ خارجی"
          ],
          "answer": 0
        },
        {
          "id": "Q005",
          "question": "سر مدل‌سازی زبانیِ پس از ستون بلوک‌ها چه عملی انجام می‌دهد؟",
          "options": [
            "گرفتن بردارِ خروجی آخر، نگاشت آن با یک ماتریسِ بازجاسازی و اعمال نرم‌افزار روی واژگان برای تولید یک توکن",
            "فشرده‌سازی تمام بردارهای خروجی به یک بردار منفرد",
            "جایگزینی تمام جاسازی‌ها با بردارهای ثابت",
            "تولید مستقیم نمایش‌های موقعیتی جدید"
          ],
          "answer": 0
        },
        {
          "id": "Q006",
          "question": "اگر هنگام خواندن جمله از چپ به راست هنوز مرجعِ ضمیر «it» روشن نشده باشد، نمایش آن در مدل چه ویژگی‌ای دارد؟",
          "options": [
            "فقط صفت‌های مربوط به یکی از مراجع ممکن را نشان می‌دهد",
            "ممکن است ترکیبی از ویژگی‌های هر دو مرجعِ احتمالی را در خود داشته باشد",
            "کاملاً برابر با بردارِ ثابت یک ضمیر است",
            "به‌طور کامل حذف و نادیده گرفته می‌شود"
          ],
          "answer": 1
        },
        {
          "id": "Q007",
          "question": "در مقایسه با جاسازی‌های ایستا مانند word2vec، نمایش‌های متنی مبتنی بر توجه چه تفاوتی دارند؟",
          "options": [
            "نمایش‌ها به متن پیرامون حساس هستند و با زمینه تغییر می‌کنند",
            "همیشه یک بردار ثابت و بدون تغییر ارائه می‌دهند",
            "تنها برای کلماتِ اسمی کار می‌کنند",
            "فقط برای زبان‌های با دستور ثابت قابل استفاده‌اند"
          ],
          "answer": 0
        },
        {
          "id": "Q008",
          "question": "ستونِ بلاک‌های ترنسفورمر چه نگاشتی را بین بردارهای ورودی و خروجی انجام می‌دهد؟",
          "options": [
            "پنجره‌ای از بردارهای ورودی را به پنجره‌ای از بردارهای خروجی با همان طول نگاشت می‌کند",
            "تمام بردارهای ورودی را به یک بردار خروجی واحد فشرده می‌کند",
            "تعداد بردارهای خروجی را افزایش می‌دهد تا طول را دو برابر کند",
            "فقط اولین بردار ورودی را به خروجی نگاشت می‌کند"
          ],
          "answer": 0
        },
        {
          "id": "Q009",
          "question": "پهنای معمولیِ ستونِ بلوک‌ها در یک ترنسفورمر چند بلوک متوالی ممکن است باشد؟",
          "options": [
            "حدود دوازده تا نود‌وشش یا بیشتر",
            "همیشه فقط یک بلوک",
            "چند هزار بلوک به طور معمول",
            "دو تا چهار بلوک معمولاً"
          ],
          "answer": 0
        },
        {
          "id": "Q010",
          "question": "مزیت اصلی مکانیزمِ توجه در پردازش زبان چیست؟",
          "options": [
            "ایجاد نمایش‌های زمینه‌ای با یکپارچه‌سازی اطلاعات از توکن‌های دوردست",
            "افزایش سرعتِ اجرای سخت‌افزار بدون بهبود نمایش‌ها",
            "حذف نیاز به هرگونه جاسازیِ ورودی",
            "محدود کردن مدل به روابطِ فقط محلی بین کلمات"
          ],
          "answer": 0
        }
      ]
    },
    {
      "chunk_id": "CHUNK_02",
      "questions": [
        {
          "id": "Q011",
          "question": "در مدل ترنسفورمر، نقش مکانیزم attention چیست؟",
          "options": [
            "ترجمهٔ توکن‌ها به زبان دیگر",
            "حذف توکن‌‌های غیرمرتبط از ورودی",
            "وزن‌دهی و ترکیب نمایش‌های توکن‌های مناسب از متن برای ساخت نمایش در لایه بعد",
            "ذخیرهٔ مکان دقیق هر توکن"
          ],
          "answer": 2
        },
        {
          "id": "Q012",
          "question": "براساس متن، چگونه می‌توان فهمید که واژه bank به معنی کنارهٔ رود/برکه است نه مؤسسهٔ مالی؟",
          "options": [
            "با توجه به کلمات زمینه‌ای مثل pond که معنا را روشن می‌کنند",
            "با بررسی قاعدهٔ صرفی-نحوی",
            "با نگاه به زمان فعل جمله",
            "با توجه به علامت‌گذاری جمله"
          ],
          "answer": 0
        },
        {
          "id": "Q013",
          "question": "چگونه در ترنسفورمرها نمایش‌های زمینه‌ای واژه‌ها به تدریج ساخته می‌شود؟",
          "options": [
            "با استفاده از بردار ثابتی که برای هر واژه همیشه یکسان است",
            "با افزودن اطلاعات از لایهٔ قبلی و ترکیب آن با اطلاعات توکن‌های همسایه در هر لایه",
            "با شمارش فراوانی کلمات در متن و ایجاد بردار از فرکانس",
            "با برچسب‌گذاری دستی معانی هر واژه"
          ],
          "answer": 1
        },
        {
          "id": "Q014",
          "question": "در مثال جمله دربارهٔ it، ترنسفورمر بیشترین توجه را به کدام توکن‌ها معطوف می‌کند؟",
          "options": [
            "pond و bank",
            "keys و are",
            "subject و verb",
            "chicken و road"
          ],
          "answer": 3
        },
        {
          "id": "Q015",
          "question": "چرا منطقی است که مدل به توکن‌های chicken و road توجه بالایی داشته باشد وقتی می‌خواهد نمایش it را بسازد؟",
          "options": [
            "چون it ممکن است به chicken یا به road ارجاع داده شود",
            "چون این کلمات املا یا شکل نوشتاری مشابهی دارند",
            "چون آن‌ها تنها اسم‌های جمله هستند",
            "چون این‌ها نزدیک‌ترین توکن‌ها به it هستند"
          ],
          "answer": 0
        },
        {
          "id": "Q016",
          "question": "در شکل توضیحی، سایهٔ تیره‌تر روی ستون‌های توجه نشانگر چه چیزی است؟",
          "options": [
            "نشانگر میزان پایین‌تر توجه",
            "نشانگر مقادیر بالاترِ وزن توجه خودی",
            "نشانگر تکرار بیشتر آن توکن در متن",
            "نشانگر فاصلهٔ مکانی بیشتر از توکن هدف"
          ],
          "answer": 1
        },
        {
          "id": "Q017",
          "question": "در هر لایه، نمایشِ یک توکن i چگونه محاسبه می‌شود؟",
          "options": [
            "با تولید یک برچسب نحوی کلی برای کل جمله",
            "با ایجاد ترجمه ماشینی از همان توکن",
            "با ترکیب اطلاعات قبلی خودِ توکن i و اطلاعاتی که از توکن‌های همسایه دریافت می‌شود",
            "با اختصاص یک بردار ثابت مستقل از متن"
          ],
          "answer": 2
        },
        {
          "id": "Q018",
          "question": "متن چه می‌گوید دربارهٔ فاصلهٔ کلمات زمینه‌ای که در فهم معنای یک واژه کمک می‌کنند؟",
          "options": [
            "آن‌ها همیشه کلمات کناری و مجاور هستند",
            "فقط در همان جملهٔ بلافصل قابل استفاده‌اند",
            "همیشه در ابتدای پاراگراف قرار دارند",
            "ممکن است در نقطه‌ای نسبتاً دور در همان جمله یا پاراگراف حضور داشته باشند"
          ],
          "answer": 3
        },
        {
          "id": "Q019",
          "question": "برای ساخت نمایش در لایهٔ بعدی (k+1)، وزن‌ها و نمایش‌ها از کدام لایه گرفته می‌شوند؟",
          "options": [
            "نمایش‌های محاسبه‌شده در لایهٔ k",
            "نمایش‌های لایهٔ نهایی مدل",
            "فقط نمایش‌های ورودی اولیه قبل از لایه‌ها",
            "نمایش‌های لایهٔ k+1 که هنوز محاسبه نشده‌اند"
          ],
          "answer": 0
        },
        {
          "id": "Q020",
          "question": "مزیت اصلی ساخت نمایش‌های زمینه‌ای برای هر واژه چیست؟",
          "options": [
            "کاهش تعداد توکن‌ها در واژه‌نامهٔ مدل",
            "ادغام معناهای کلمات زمینه‌ای برای فهم بهتر معنای واژه در آن بستر",
            "افزایش سرعت اجرای سخت‌افزار",
            "تحمیل توافق عددی بین فاعل و فعل"
          ],
          "answer": 1
        }
      ]
    },
    {
      "chunk_id": "CHUNK_03",
      "questions": [
        {
          "id": "Q021",
          "question": "وظیفه محاسبه توجه در یک لایه ترنسفورمر چیست؟",
          "options": [
            "ساخت نمای برداری از یک توکن در آن لایه",
            "پیش‌بینی احتمال دقیق کلمه بعدی",
            "محاسبه مقدار تابع خطا برای آن لایه",
            "تولید یک بردار یکتا برای کل رشته ورودی"
          ],
          "answer": 0
        },
        {
          "id": "Q022",
          "question": "برای تولید خروجی در یک موقعیت معین، مکانیزم توجه چه ورودی‌هایی را می‌گیرد؟",
          "options": [
            "فقط نمایش توکن فعلی در آن موقعیت",
            "فقط نمایش همه توکن‌های قبلی بدون نمایش توکن فعلی",
            "نمایش توکن فعلی همراه با نمایش‌های توکن‌های قبلی در پنجره زمینه",
            "فقط نمایش توکن‌های آینده"
          ],
          "answer": 2
        },
        {
          "id": "Q023",
          "question": "در مدل‌های علّی که از چپ به راست کار می‌کنند، پنجره زمینه عمدتاً شامل چه محتواهایی است؟",
          "options": [
            "تمام توکن‌ها از جمله توکن‌های آینده",
            "فقط کلمات یا توکن‌های قبلی",
            "تنها توکن فعلی",
            "فقط توکن‌هایی که بعد از موقعیت فعلی قرار دارند"
          ],
          "answer": 1
        },
        {
          "id": "Q024",
          "question": "چه محدودیتی دربارهٔ دسترسی مدل هنگام پردازش یک موقعیت فعلی وجود دارد؟",
          "options": [
            "مدل تنها به نمایش همان موقعیت دسترسی دارد",
            "مدل می‌تواند به تمام توکن‌های آینده نگاه کند",
            "مدل فقط به اولین توکن جمله دسترسی دارد",
            "مدل به نمایش توکن‌های قبلی دسترسی دارد اما نه به توکن‌های بعد از موقعیت فعلی"
          ],
          "answer": 3
        },
        {
          "id": "Q025",
          "question": "یک لایه خودتوجه چگونه دنباله ورودی را به خروجی تبدیل می‌کند؟",
          "options": [
            "تولید دنباله‌ای خروجی با همان طول دنباله ورودی",
            "فشرده‌سازی همه ورودی‌ها به یک بردار نهایی",
            "حذف بعضی توکن‌ها برای کوتاه کردن دنباله",
            "افزایش طول دنباله با اضافه کردن توکن‌های جدید"
          ],
          "answer": 0
        },
        {
          "id": "Q026",
          "question": "در توضیح ساده‌شده، خروجی توجه در اصل از چه ترکیبی به‌دست می‌آید؟",
          "options": [
            "یک ترکیب غیرخطی پیچیده از نمایه‌ها",
            "انتخاب تنها نمایه‌ای که بیشترین تکرار را دارد",
            "جمعی وزن‌دار از بردارهای زمینه",
            "میانگین ساده و بدون وزن از همه بردارها"
          ],
          "answer": 2
        },
        {
          "id": "Q027",
          "question": "در نسخه ساده‌شده، وزن مربوط به هر بردار زمینه چگونه تعیین می‌شود؟",
          "options": [
            "بر مبنای موقعیت مکانی توکن‌ها در دنباله",
            "متناسب با میزان شباهت بین بردار توکن جاری و بردار توکن قبلی",
            "به‌صورت تصادفی انتخاب می‌شود",
            "بر اساس طول کاراکترهای توکن"
          ],
          "answer": 1
        },
        {
          "id": "Q028",
          "question": "برای محاسبه امتیاز شباهت بین دو نمایش برداری در متن، از چه عملیاتی استفاده می‌شود؟",
          "options": [
            "حاصل‌ضرب داخلی بردارها",
            "محاسبه فاصله زمانی بین توکن‌ها",
            "مرتب‌سازی بردارها بر اساس اندازه مطلق",
            "ساخت گراف اتصالات بین توکن‌ها"
          ],
          "answer": 0
        },
        {
          "id": "Q029",
          "question": "پس از به‌دست آوردن امتیازهای شباهت خام، چه گامی برای تولید وزن‌های نهایی برداشته می‌شود؟",
          "options": [
            "حذف امتیازهای منفی و نگه داشتن بقیه",
            "تقسیم امتیازها بر طول جمله",
            "انتخاب تنها بهترین امتیاز و صفر کردن سایر امتیازها",
            "نرمال‌سازی امتیازها با استفاده از تابع سافت‌مکس"
          ],
          "answer": 3
        },
        {
          "id": "Q030",
          "question": "برای محاسبه خروجی در موقعیت سوم در نمونه ساده‌شده، کدام فرایند مطابق توضیحات است؟",
          "options": [
            "گرفتن میانگین برابر از نمایش‌های سه موقعیت اول تا سوم بدون توجه به شباهت",
            "محاسبه امتیاز شباهت بین نمایش موقعیت سوم و هر یک از نمایش‌های قبلی و خودش، نرمال‌سازی این امتیازها و سپس جمع وزن‌دار نمایش‌ها",
            "استفاده تنها از نمایش مربوط به موقعیت سوم",
            "نگاه کردن به نمایش توکن‌های پس از موقعیت سوم برای تعیین وزن‌ها"
          ],
          "answer": 1
        }
      ]
    },
    {
      "chunk_id": "CHUNK_04",
      "questions": [
        {
          "id": "Q031",
          "question": "چرا وزن نرم‌ماکس معمولاً برای بردار یک موقعیت بیشترین مقدار را دارد؟",
          "options": [
            "چون آن بردار معمولاً شبیه‌ترین بردار به خودش است و بنابراین شباهت بالایی دارد",
            "چون مدل به طور صریح یک بایاس برای بردار جاری می‌آموزد",
            "چون بردار جاری همیشه از جلوترین موقعیت استفاده می‌کند",
            "چون نرم‌ماکس مکان بردارها را تغییر می‌دهد"
          ],
          "answer": 0
        },
        {
          "id": "Q032",
          "question": "در فرایند توجه، هر بردار ورودی چه سه نقش متمایزی می‌تواند داشته باشد؟",
          "options": [
            "نمایش، موقعیت، وزن",
            "پیش‌بینی، بازخورد، خطا",
            "پرسش، کلید، مقدار",
            "ورودی، خروجی، میانگین"
          ],
          "answer": 2
        },
        {
          "id": "Q033",
          "question": "برای بازنمایی مستقل نقش‌های پرسش، کلید و مقدار از چه روشی استفاده می‌شود؟",
          "options": [
            "هر بردار ورودی با ماتریس‌های وزن جداگانه به نمایش‌های مربوط به هر نقش نگاشت می‌یابد",
            "از همان بردار ورودی برای هر نقش بدون تغییر استفاده می‌شود",
            "نقش‌ها با اضافه‌کردن نویز تصادفی به بردار اصلی جدا می‌شوند",
            "با استفاده از فیلترهای کانولوشنی محتوای نقش‌ها استخراج می‌شود"
          ],
          "answer": 0
        },
        {
          "id": "Q034",
          "question": "چگونه شباهت بین عنصر جاری و یک عنصر قبلی معمولاً محاسبه می‌شود؟",
          "options": [
            "با اندازه‌گیری فاصله هندسی بدون تبدیل بردارها",
            "با محاسبه اختلاف در مقادیر شاخص موقعیت",
            "با مقایسه ترتیب قرارگیری در دنباله",
            "با ضرب داخلی بین نمایش پرسش عنصر جاری و نمایش کلید عنصر قبلی"
          ],
          "answer": 3
        },
        {
          "id": "Q035",
          "question": "علت مقیاس‌بندی مقدار حاصل از ضرب داخلی پیش از اعمال نرم‌ماکس چیست؟",
          "options": [
            "برای جلوگیری از تولید مقادیر خیلی بزرگ که می‌تواند باعث ناپایداری عددی و از بین رفتن گرادیان‌ها شود",
            "برای تضمین اینکه خروجی همیشه بین صفر و یک باشد",
            "برای اجباری کردن توزیع یکنواخت وزن‌ها",
            "برای حذف مقادیر منفی قبل از نرم‌ماکس"
          ],
          "answer": 0
        },
        {
          "id": "Q036",
          "question": "عملکرد نرم‌ماکس در محاسبه توجه چه چیزی را تولید می‌کند؟",
          "options": [
            "یک بردار خروجی جدید برای عنصر جاری",
            "مجموع مقادیر وزن‌دار عناصر قبلی",
            "یک توزیع احتمال که به عنوان وزن‌ها برای جمع مقادیر استفاده می‌شود",
            "فهرستی از اندیس‌های بیشترین شباهت"
          ],
          "answer": 2
        },
        {
          "id": "Q037",
          "question": "چطور بردار خروجی خودتوجه برای یک عنصر تولید می‌شود؟",
          "options": [
            "با میانگین ساده تمام بردارهای ورودی",
            "با جمع مقادیر عناصر قبلی که هر کدام به وسیله وزن شباهت‌شان ضرب شده‌اند و سپس پردازش خروجی",
            "با انتخاب برداری که بیشترین شباهت را دارد و بازگرداندن همان بردار",
            "با الحاق بردارهای پرسش و کلید"
          ],
          "answer": 1
        },
        {
          "id": "Q038",
          "question": "چه مزیتی از عنوان کردن یک لایه به صورت «هد» در توجه حاصل می‌شود؟",
          "options": [
            "هد باعث کاهش تعداد پارامترها می‌شود",
            "هد به مدل اجازه می‌دهد که فقط به عنصر پیشین مستقیم توجه کند",
            "هد نشان‌دهنده استفاده از فعل و انفعالات کانولوشنی است",
            "هد امکان بازنمایی مجزا و سازمان‌یافته نقش‌های مختلف هر ورودی را فراهم می‌آورد"
          ],
          "answer": 3
        },
        {
          "id": "Q039",
          "question": "در نسخه خودتوجهی که در قطعه متن آمده، هنگام محاسبه توجه برای موقعیت i از کدام عناصر استفاده می‌شود؟",
          "options": [
            "فقط عناصر تا و شامل عنصر جاری (عناصر قبلی و خودِ i)",
            "تمامی عناصر در کل دنباله بدون توجه به موقعیت",
            "تنها عنصر قبلی فوری",
            "فقط عناصر بعدی در دنباله"
          ],
          "answer": 0
        },
        {
          "id": "Q040",
          "question": "چرا به جای استفاده مستقیم از بردار ورودی برای همه نقش‌ها، از نگاشت جداگانه برای هر نقش استفاده می‌کنند؟",
          "options": [
            "برای کاهش پیچیدگی محاسباتی مدل",
            "برای حفظ تعداد پارامترها ثابت",
            "زیرا یک بردار واحد نمی‌تواند هم‌زمان نمایش‌های مناسب برای مقایسه و بازیابی اطلاعات را ارائه دهد",
            "برای جلوگیری از هم‌پوشانی موقعیت‌های زمانی"
          ],
          "answer": 2
        }
      ]
    },
    {
      "chunk_id": "CHUNK_05",
      "questions": [
        {
          "id": "Q041",
          "question": "در معماری توجه، افزودن ماتریس WO چه کاربردی دارد؟",
          "options": [
            "تغییر شکل خروجی سر توجه به گونه‌ای که با بعد مدل همسان شود",
            "محاسبه مستقیم وزن‌های توجه بین پرسش و کلیدها",
            "ایجاد بردارهای پرسش، کلید و مقدار از ورودی",
            "کاهش نویز اطلاعاتی قبل از اعمال چندسر توجه"
          ],
          "answer": 0
        },
        {
          "id": "Q042",
          "question": "یکی از دلایل نگه داشتن همان بعد برای ورودی و خروجی توجه در ترنسفورمر چیست؟",
          "options": [
            "برای کمینه‌سازی حافظه مصرفی مدل",
            "افزایش ماژولار بودن و سازگاری اجزای مدل",
            "افزایش سرعت محاسبات ضرب درونی بین بردارها",
            "اجتناب از نیاز به ماتریس‌های تبدیل میانی"
          ],
          "answer": 1
        },
        {
          "id": "Q043",
          "question": "بردار پرسش و بردار کلید دارای چه خصوصیتی هستند که اجازه می‌دهد ضرب درونی آنها یک مقدار اسکالر تولید کند؟",
          "options": [
            "هر یک دارای بعد dv هستند و ضرب عنصری تولید می‌کنند",
            "یکی بردار داده و دیگری ماتریس وزن است که با هم ضرب می‌شوند",
            "هر دو بردار دارای همان بعدی به نام dk هستند و ضرب درونی آنها اسکالر می‌سازد",
            "بردار پرسش طولانی‌تر است و مقیاس‌پذیری را افزایش می‌دهد"
          ],
          "answer": 2
        },
        {
          "id": "Q044",
          "question": "قبل از اعمال ماتریس WO، خروجی یک سر توجه چه بعد مفهومی‌ای دارد؟",
          "options": [
            "همان بعد کلی مدل d",
            "بعد برابر با تعداد سرها",
            "بعدی برابر با طول ورودی",
            "بعدی که با dv مشخص می‌شود"
          ],
          "answer": 3
        },
        {
          "id": "Q045",
          "question": "در مثال ذکرشده از کار اولیه ترنسفورمر، چه ترکیب عددی برای ابعاد d، dk و dv آورده شده بود؟",
          "options": [
            "d برابر 256 و dk و dv هر کدام 32",
            "d برابر 512 و dk و dv هر دو 64",
            "d برابر 1024 و dk برابر 128 و dv برابر 64",
            "d برابر 512 و dk برابر 512 و dv برابر 512"
          ],
          "answer": 1
        },
        {
          "id": "Q046",
          "question": "هدف اصلی استفاده از چند سر توجه موازی در یک لایه چیست؟",
          "options": [
            "هر سر بتواند به جنبه‌ها یا الگوهای متفاوتی از زمینه توجه کند",
            "کاهش کلی تعداد پارامترهای مدل",
            "افزایش سرعت اجرا با کاهش محاسبات داخل هر سر",
            "جلوگیری از هم‌پوشانی اطلاعات بین توکن‌ها"
          ],
          "answer": 0
        },
        {
          "id": "Q047",
          "question": "ماتریس‌های WQ، WK و WV چه نقشی در فرایند توجه ایفا می‌کنند؟",
          "options": [
            "آنها وزن‌های خروجی نهایی را ترکیب می‌کنند",
            "آنها برای محاسبه ماتریس WO به کار می‌روند",
            "آنها ورودی را به بردارهای پرسش، کلید و مقدار تبدیل می‌کنند",
            "آنها طول توالی را تغییر می‌دهند تا همگام با سرها شوند"
          ],
          "answer": 2
        },
        {
          "id": "Q048",
          "question": "علاوه بر ورودی و خروجی توجه، کدام اجزا نیز در متن اشاره شده‌اند که همان بعد مدل را دارند؟",
          "options": [
            "فقط وزن‌های ماتریسی در لایه توجه",
            "خروجی هر بلوک ترنسفورمر و بردارهای میانی داخل بلاک",
            "تنها بردارهای مقدار در هر سر توجه",
            "تنها سرهای توجه منفرد"
          ],
          "answer": 1
        },
        {
          "id": "Q049",
          "question": "وقتی گفته می‌شود چند سر توجه جداگانه وجود دارد که هر کدام پارامترهای خود را دارند، منظور چیست؟",
          "options": [
            "هر سر توجه یک نسخه فشرده از کل مدل را دارد",
            "سرها پارامترهای خود را به صورت اشتراکی نگه می‌دارند",
            "هر سر به طور مستقل پارامترهایی دارد تا جنبه‌های متفاوتی از روابط ورودی را مدلسازی کند",
            "تنها یک سر پارامتر مستقل دارد و بقیه بردارهای آن را کپی می‌کنند"
          ],
          "answer": 2
        },
        {
          "id": "Q050",
          "question": "سرهای توجه موازی که در متن ذکر شده‌اند در چه سطحی از مدل قرار می‌گیرند؟",
          "options": [
            "در لایه ابتدایی مدل تنها",
            "در لایه پایانی مدل تنها",
            "در عمق‌های متفاوت و پراکنده در سراسر مدل",
            "در همان عمق و به‌صورت موازی در آن نقطه قرار می‌گیرند"
          ],
          "answer": 3
        }
      ]
    },
    {
      "chunk_id": "CHUNK_06",
      "questions": [
        {
          "id": "Q051",
          "question": "در یک لایه خودتوجه چندسری، هر سر معمولاً چه مجموعه‌ای از ماتریس‌های وزنی دارد؟",
          "options": [
            "فقط یک ماتریس خروجی مشترک بین همه سرها",
            "ماتریس‌های مخصوص فقط برای پرسش و خروجی",
            "مجموعه‌ای از ماتریس‌های پرسش، کلید و مقدار مخصوص همان سر",
            "ماتریس‌های مربوط به لایه نرمال‌سازی"
          ],
          "answer": 2
        },
        {
          "id": "Q052",
          "question": "هدف از استفاده از ماتریس‌های پرسش، کلید و مقدار برای هر سر چیست؟",
          "options": [
            "پروژه‌کردن ورودی‌ها به بردارهای پرسش، کلید و مقدار مخصوص آن سر",
            "جمع کردن خروجی همه سرها بدون تبدیل",
            "محاسبه‌ی وزن‌های به‌روزرسانی پارامترها",
            "اعمال نرمال‌سازی روی بردارها"
          ],
          "answer": 0
        },
        {
          "id": "Q053",
          "question": "هنگامی که از چند سر استفاده می‌شود، کدام بعد برای بردار ورودی و خروجی مدل حفظ می‌شود؟",
          "options": [
            "بعد بردار پرسش و کلید",
            "بعد مدل که نماینده طول بردار ورودی و خروجی است",
            "بعد مقدار",
            "تعداد سرها"
          ],
          "answer": 1
        },
        {
          "id": "Q054",
          "question": "در مقاله‌ی اصلی ترنسفورمر چه مقادیری برای ابعاد dk و dv، تعداد سرها و بعد مدل ذکر شده‌اند؟",
          "options": [
            "dk و dv برابر 128، تعداد سرها 4، بعد مدل 256",
            "dk و dv برابر 32، تعداد سرها 16، بعد مدل 1024",
            "dk برابر 64 و dv برابر 128، تعداد سرها 8، بعد مدل 512",
            "dk و dv هر دو برابر 64، تعداد سرها 8، بعد مدل 512"
          ],
          "answer": 3
        },
        {
          "id": "Q055",
          "question": "در حالت توجه علی یا چپ‌به‌راست که در این فصل استفاده می‌شود، «سایر ورودی‌ها» برای هر موقعیت کدام توکن‌ها هستند؟",
          "options": [
            "فقط توکن‌هایی که در موقعیت‌های قبلی (چپ) قرار دارند",
            "تنها توکن‌های بعدی (به راست)",
            "تمام توکن‌های جمله بدون توجه به ترتیب",
            "فقط توکن‌های غیرمجاور"
          ],
          "answer": 0
        },
        {
          "id": "Q056",
          "question": "پس از اینکه خروجی هر سر محاسبه شد، چه روندی برای تبدیل آن به خروجی نهایی مدل انجام می‌شود؟",
          "options": [
            "هر خروجی سر به طور مستقل به بعد مدل نگاشت زده می‌شود و سپس جمع می‌شود",
            "فقط خروجی سر اول به عنوان خروجی نهایی انتخاب می‌شود",
            "خروجی‌های همه سرها کنار هم قرار می‌گیرند و سپس با یک نگاشت خطی به بعد مدل بازنگاشت می‌شوند",
            "خروجی‌ها فشرده شده و به صورت میانگین گرفته می‌شوند"
          ],
          "answer": 2
        },
        {
          "id": "Q057",
          "question": "بلاک ترنسفورمر علاوه بر لایه خودتوجه شامل چه اجزایی است؟",
          "options": [
            "فقط لایه‌های کانولوشن و دراپ‌اوت",
            "یک لایه فیدفوروارد، اتصال‌های باقیمانده و لایه‌های نرمال‌سازی",
            "چندین لایه بازگشتی همراه با حافظه بلندمدت",
            "تنها یک لایه خطی اضافی"
          ],
          "answer": 1
        },
        {
          "id": "Q058",
          "question": "دیدگاه جریان باقیمانده درباره پردازش یک توکن چه تصویری ارائه می‌دهد؟",
          "options": [
            "پردازش توکن‌ها به صورت جداگانه و بدون اشتراک اطلاعات",
            "یک سلسله مراتب چندجمله‌ای از لایه‌ها که اطلاعات را فشرده می‌کند",
            "فقط مسیر مستقیم از ورودی به خروجی بدون افزوده شدن اطلاعات",
            "یک جریان واحد از نمایش‌های با بعد مدل که اجزا خروجی‌شان را می‌خوانند و به آن اضافه می‌کنند"
          ],
          "answer": 3
        },
        {
          "id": "Q059",
          "question": "ورودی اولیه‌ای که در پایین جریان باقیمانده قرار می‌گیرد معمولاً چیست؟",
          "options": [
            "یک بردار جاسازی برای همان توکن که دارای بعد مدل است",
            "یک بردار صفر برای همه توکن‌ها",
            "نمایش میانگین تمام توکن‌های جمله",
            "یک بردار تصادفی که در طول آموزش تغییر نمی‌کند"
          ],
          "answer": 0
        },
        {
          "id": "Q060",
          "question": "آیا پرسش‌ها، کلیدها و مقادیر برای همه سرها با همان ماتریس‌ها محاسبه می‌شوند یا هر سر مجموعه‌ای جداگانه دارد؟",
          "options": [
            "همه سرها از یک مجموعه ماتریس مشترک استفاده می‌کنند",
            "فقط کلیدها و مقادیر برای هر سر جدا هستند، پرسش‌ها مشترک است",
            "هر سر مجموعه‌ای جداگانه از ماتریس‌های پرسش، کلید و مقدار دارد",
            "فقط ماتریس خروجی برای هر سر مجزا است"
          ],
          "answer": 2
        }
      ]
    },
    {
      "chunk_id": "CHUNK_07",
      "questions": [
        {
          "id": "Q061",
          "question": "کدام توصیف درباره لایه فیدفوروارد در یک بلوک ترنسفورمر درست است؟",
          "options": [
            "شبکه‌ای کاملاً متصل با دو لایه و یک لایه پنهان است که وزن‌ها بین موقعیت‌های توکنی یکسان اما بین لایه‌ها متفاوتند",
            "وزن‌های آن برای هر موقعیت توکنی متفاوت است و بین لایه‌ها یکسان می‌ماند",
            "ابعاد لایه پنهان همیشه برابر با بعد مدل است",
            "لایه فیدفوروارد همان مکانیزم توجه چندسر است"
          ],
          "answer": 0
        },
        {
          "id": "Q062",
          "question": "نرمال‌سازی لایه (لِیینرمال) در ترنسفورمر روی چه چیزی اعمال می‌شود؟",
          "options": [
            "روی همه توکن‌های موجود در یک ورودی به‌صورت هم‌زمان",
            "روی برداری که نمایانگر یک توکن منفرد است",
            "روی پارامترهای وزن یک لایه",
            "روی خروجی نهایی تمام بلوک‌ها"
          ],
          "answer": 1
        },
        {
          "id": "Q063",
          "question": "هدف اصلی اجرای نرمال‌سازی لایه در شبکه‌های عمیق چیست؟",
          "options": [
            "کاهش تعداد پارامترها در مدل",
            "تبدیل بردارها به بردارهای دارای توزیع گوسی دقیق",
            "قرار دادن مقادیر یک بردار در بازه‌ای که آموزش مبتنی بر گرادیان را تسهیل کند و سپس اعمال ضریب و بایاس قابل یادگیری",
            "جایگزینی مکانیزم توجه برای انتقال اطلاعات بین توکن‌ها"
          ],
          "answer": 2
        },
        {
          "id": "Q064",
          "question": "در یک بلوک ترنسفورمر استاندارد، چند بار نرمال‌سازی لایه انجام می‌شود داخل همان بلوک (قبل از نرمال‌سازی نهایی در انتهای شبکه)؟",
          "options": [
            "یک بار",
            "سه بار",
            "چهار بار",
            "دو بار"
          ],
          "answer": 3
        },
        {
          "id": "Q065",
          "question": "کدام جزءِ بلوک ترنسفورمر اطلاعات را از توکن‌های دیگر دریافت می‌کند؟",
          "options": [
            "مکانیزم توجه چندسر",
            "شبکه فیدفوروارد دو لایه",
            "نرمال‌سازی لایه",
            "جمع با مسیر رزیدوال"
          ],
          "answer": 0
        },
        {
          "id": "Q066",
          "question": "طبق توضیحات، جریان رزیدوال در لایه‌های ابتدایی و در لایه‌های بالاتر معمولاً چه نمایشی از توکن‌ها دارد؟",
          "options": [
            "در همه لایه‌ها نمایانگر توکن بعدی است",
            "در لایه‌های ابتدایی نمایانگر توکن جاری و در لایه‌های بالاتر معمولاً نمایانگر توکن بعدی است",
            "همیشه فقط اطلاعات همسایه‌ها را نگه می‌دارد",
            "در لایه‌های بالاتر هیچ نمایشی از توکن‌ها وجود ندارد"
          ],
          "answer": 1
        },
        {
          "id": "Q067",
          "question": "چرا خروجی و ورودی هر بلوک ترنسفورمر دارای همان بعد برداری هستند؟",
          "options": [
            "برای کاهش محاسبات ماتریسی",
            "برای جدا کردن فضای تعبیه‌بندی به زیربخش‌های مستقل",
            "تا امکان قرار دادن چند بلوک پشت سر هم و ساختن مدل‌های چندلایه فراهم شود",
            "به‌خاطر محدودیت‌های توجه چندسر"
          ],
          "answer": 2
        },
        {
          "id": "Q068",
          "question": "چطور می‌توان نقش یک سر توجه را در انتقال اطلاعات بین توکن‌ها توصیف کرد؟",
          "options": [
            "سر توجه فقط وزن‌های نرمال‌سازی را محاسبه می‌کند",
            "سر توجه بردار توکن فعلی را حذف می‌کند و جایگزین می‌سازد",
            "سر توجه اطلاعات را به‌صورت کامل به توکن مقصد کپی می‌کند",
            "سر توجه می‌تواند اطلاعات را از جریان رزیدوال یک توکن همسایه به جریان توکن جاری منتقل کند"
          ],
          "answer": 3
        },
        {
          "id": "Q069",
          "question": "در ترتیب عملیات یک بلوک ترنسفورمر، لایه فیدفوروارد بعد از کدام مرحله اعمال می‌شود؟",
          "options": [
            "پس از نرمال‌سازی لایه دوم که به دنبال جمع رزیدوال آمده است",
            "بلافاصله پس از لایه نرمال‌سازی اول",
            "قبل از اعمال مکانیزم توجه",
            "پس از اجرای نرمال‌سازی نهایی در انتهای شبکه"
          ],
          "answer": 0
        },
        {
          "id": "Q070",
          "question": "در عمل، مدل‌های بزرگ زبانی معمولاً چند بلوک ترنسفورمر را روی هم قرار می‌دهند؟",
          "options": [
            "همیشه تنها یک بلوک",
            "از حدود دوازده بلوک تا ده‌ها بلوک بسته به اندازه مدل",
            "به‌طور ثابت دوازده بلوک برای تمام مدل‌ها",
            "همیشه بیش از هزار بلوک"
          ],
          "answer": 1
        }
      ]
    },
    {
      "chunk_id": "CHUNK_08",
      "questions": [
        {
          "id": "Q071",
          "question": "هدف از قرار دادن همهٔ بردارهای ورودی در یک ماتریس X چیست؟",
          "options": [
            "برای کاهش تعداد لایه‌های ترنسفورمر",
            "برای ذخیرهٔ کمتر حافظه در هنگام آموزش",
            "برای امکان پردازش موازی و بهره‌گیری از ضرب ماتریسی کارا",
            "برای افزایش طول بردارهای جاسازی"
          ],
          "answer": 2
        },
        {
          "id": "Q072",
          "question": "چرا محاسبهٔ توجه برای هر توکن را می‌توان مستقل از بقیه در نظر گرفت؟",
          "options": [
            "زیرا محاسبات برای هر توکن وابسته به توکن بعدی است",
            "زیرا عملیات لازم برای هر توکن جداگانه بوده و قابل موازی‌سازی است",
            "زیرا فقط در سرِ آخر مدل مستقل است",
            "زیرا فقط در مدل‌های چندسره این استقلال برقرار است"
          ],
          "answer": 1
        },
        {
          "id": "Q073",
          "question": "برای ترنسفورمرهای معمولی طول ورودی N معمولاً در چه بازه‌ای قرار دارد؟",
          "options": [
            "بین ده تا صد",
            "همیشه کمتر از هزار",
            "دقیقاً برابر با یک میلیون",
            "معمولاً از حدود هزار تا سی‌و‌دو هزار"
          ],
          "answer": 3
        },
        {
          "id": "Q074",
          "question": "وقتی ماتریس X را با ماتریس‌های مربوط به کوئری، کلید و مقدار ضرب می‌کنیم، هر یک از ماتریس‌های حاصل چه چیزی در بردارند؟",
          "options": [
            "مجموعهٔ بردارهای کوئری، کلید یا مقدار برای تمام توکن‌ها",
            "بردارهای خروجی نهایی مدل برای هر توکن",
            "وزن‌های به‌روز‌رسانی گرادیان‌ها",
            "بردارهای موقعیت زمانی توکن‌ها"
          ],
          "answer": 0
        },
        {
          "id": "Q075",
          "question": "ضرب ماتریسی بین ماتریس‌های کوئری و ترانهادهٔ کلید چه اطلاعاتی را به‌دست می‌دهد؟",
          "options": [
            "مجموع نهایی توجه پس از نرم‌سازی",
            "نتیجهٔ ضربِ مقادیری که جایگزین بردارهای مقدار می‌شوند",
            "ماتریسی از مقایسه‌های جفتی بین هر کوئری و هر کلید برای تمام توکن‌ها",
            "نمایش ترتیبی از موقعیت‌های توکن‌ها"
          ],
          "answer": 2
        },
        {
          "id": "Q076",
          "question": "برای رسیدن به زمینه‌های خیلی طولانی‌تر از مقادیر معمول مثل صدها هزار توکن، چه کاری لازم است؟",
          "options": [
            "تنها افزایش بعد جاسازی کافی است",
            "استفاده از یک سر توجه منفرد",
            "پاک کردن ماتریس X بین هر لایه",
            "اعمال تغییرات معماری مانند مکانیزم‌های مخصوص زمینهٔ طولانی"
          ],
          "answer": 3
        },
        {
          "id": "Q077",
          "question": "پس از محاسبهٔ ماتریس مقایسهٔ کوئری‌ها و کلیدها چه گام‌هایی پیش می‌آید تا بردار نمایش هر توکن تولید شود؟",
          "options": [
            "مقیاس‌گذاری امتیازها، اعمال نرم‌افزار نرم‌سازی، و سپس ضرب در ماتریس مقدار",
            "مستقیماً استفاده از ماتریس مقایسه به‌عنوان خروجی نهایی",
            "جمع‌کردن کوئری و کلید و سپس ارسال به لایهٔ بازخور",
            "اول ضرب در مقدار و سپس اعمال نرم‌افزار نرم‌سازی"
          ],
          "answer": 0
        },
        {
          "id": "Q078",
          "question": "در توضیح نویسنده، ترتیب بررسی توجه به چه صورت است؟",
          "options": [
            "ابتدا شرح چند سر، سپس تبدیل به تک‌سر",
            "ابتدا یک سر توجه را بررسی کرده و بعد سرهای متعدد را مطرح می‌کنند",
            "فقط حالت چند سر توضیح داده می‌شود و تک‌سر بررسی نمی‌شود",
            "مستقیماً کل بلوک ترنسفورمر بدون اشاره به سرها شرح داده می‌شود"
          ],
          "answer": 1
        },
        {
          "id": "Q079",
          "question": "در ماتریس X هر سطر نشان‌دهندهٔ چه چیزی است؟",
          "options": [
            "بردار جاسازی یک توکن از ورودی",
            "وزن‌های توجه بین دو توکن",
            "مجموع کلیدها و کوئری‌ها",
            "نتیجهٔ ضرب QK ترانهاده"
          ],
          "answer": 0
        },
        {
          "id": "Q080",
          "question": "کاهش کل مرحلهٔ خودتوجه برای دنباله‌ای از توکن‌ها به ضرب ماتریسی چه مزیتی فراهم می‌آورد؟",
          "options": [
            "حذف نیاز به لایه‌های باقیمانده",
            "امکان محاسبهٔ هم‌زمان و کارا برای تمام توکن‌ها",
            "جایگزینی نرم‌افزار نرم‌سازی با عملیات خطی ساده",
            "کاهش تعداد توکن‌های ورودی مورد نیاز"
          ],
          "answer": 1
        }
      ]
    },
    {
      "chunk_id": "CHUNK_09",
      "questions": [
        {
          "id": "Q081",
          "question": "هدف اصلی استفاده از ماسک در محاسبهٔ توجهِ خودی (self-attention) چیست؟",
          "options": [
            "جلوگیری از دسترسی مدل به توکن‌هایی که بعد از توکن فعلی می‌آیند",
            "افزایش سرعت ضرب ماتریسی در لایه توجه",
            "کاهش تعداد سرهای چندسرّی برای سبک‌تر کردن مدل",
            "به هم‌زدن ترتیب توکن‌ها برای تقویت یادگیری"
          ],
          "answer": 0
        },
        {
          "id": "Q082",
          "question": "برای بی‌اثر کردن تأثیر توکن‌های بعدی در مقایسه‌های پرسش-کلید چگونه عمل می‌شود؟",
          "options": [
            "جایگزینی آن‌ها با صفر معمولی",
            "تنظیم عناصر بالاتِر مثلثی ماتریس به مقدار منفی بی‌نهایت تا نرم‌افزار نرم‌افزاری آن‌ها را صفر کند",
            "افزودن نویز تصادفی تا وزن‌ها ناپایدار شوند",
            "ضرب کردن آن بخش از ماتریس در یک ضریب کوچک"
          ],
          "answer": 1
        },
        {
          "id": "Q083",
          "question": "چرا در مسئلهٔ مدل‌سازی زبان قرار دادن اطلاعات آینده در محاسبهٔ توجه مشکل‌ساز است؟",
          "options": [
            "چون باعث افزایش اندازهٔ ماتریس وزن‌ها می‌شود",
            "زیرا استفاده از اطلاعات آینده منجر به ناپایداری عددی می‌شود",
            "زیرا اگر مدل بتواند به کلمات آینده دسترسی داشته باشد، حدس کلمهٔ بعدی به‌شدت ساده و تقلب‌آمیز خواهد شد",
            "به خاطر آنکه اثرات متناظر را در لایه‌های بالاتر حذف می‌کند"
          ],
          "answer": 2
        },
        {
          "id": "Q084",
          "question": "پیچیدگی محاسباتی توجه نسبت به طول توالی ورودی چگونه افزایش می‌یابد؟",
          "options": [
            "بصورت خطی با طول توالی افزایش می‌یابد",
            "به صورت ثابت مستقل از طول توالی است",
            "به صورت لگاریتمی با طول توالی رشد می‌کند",
            "به صورت مربعی نسبت به طول توالی افزایش پیدا می‌کند"
          ],
          "answer": 3
        },
        {
          "id": "Q085",
          "question": "در مکانیزم چندسرّی، خروجی‌های سرها چگونه برای ادامهٔ پردازش ترکیب می‌شوند؟",
          "options": [
            "جمع برداری سرها و سپس اعمال یک لایهٔ غیرخطی",
            "میانگین‌گیری از خروجی سرها و ارسال به لایهٔ بعدی",
            "به‌هم‌چسباندن ماتریس‌های خروجی همهٔ سرها و سپس اعمال یک نگاشت خطی نهایی",
            "ضرب المان‌به‌المان خروجی سرها برای فشرده‌سازی"
          ],
          "answer": 2
        },
        {
          "id": "Q086",
          "question": "کدام گزاره درست دربارهٔ ابعاد در توجه چندسرّی است؟",
          "options": [
            "ابعاد ورودی و خروجی یک لایهٔ توجه برابر بعد کلی مدل است، در حالی که بردارهای کلید، پرسش و مقدار ابعادی مستقل دارند",
            "ابعاد ورودی همیشه بزرگتر از بعد مدل است",
            "ابعاد کلید و پرسش با بعد خروجی برابر هستند و همه یک بعد دارند",
            "ابعاد مقدار همواره صفر است تا صرفه‌جویی حافظه شود"
          ],
          "answer": 0
        },
        {
          "id": "Q087",
          "question": "خروجی هر سر در یک لایهٔ چندسرّی چطور سازمان‌دهی می‌شود نسبت به تعداد توکن‌ها و بعد بردار مقدار؟",
          "options": [
            "یک بردار منفرد مستقل از تعداد توکن‌ها",
            "یک ماتریس که ردیف‌های آن مطابق تعداد توکن‌ها و ستون‌های آن مطابق بعد بردار مقدار است",
            "یک اسکالر برای هر سر که خلاصه‌ای از توکن‌هاست",
            "یک لیست بدون ساختار از بردارهای با اندازهٔ برابر بعد مدل"
          ],
          "answer": 1
        },
        {
          "id": "Q088",
          "question": "نقش لایهٔ خطی نهایی که روی ترکیب خروجی سرها اعمال می‌شود چیست؟",
          "options": [
            "اعمال ماسک روی خروجی برای حذف اطلاعات آینده",
            "انجام نرمال‌سازی لایه‌ای قبل از جمع با ورودی اصلی",
            "افزایش طول توالی برای لایه‌های بعدی",
            "بازآرایی و تبدیل برداری خروجیِ چسبیده به بُعد اصلی مدل برای هر توکن"
          ],
          "answer": 3
        },
        {
          "id": "Q089",
          "question": "معادلاتی که با افزودن خروجی توجه چندسرّی به ورودی و سپس اعمال شبکهٔ تغذیهٔ تماماً متصل نشان داده می‌شوند چه ساختاری را نمایش می‌دهند؟",
          "options": [
            "استفاده از اتصال باقیمانده (رزیدو) و اعمال نرمال‌سازی لایه قبل از هر زیرلایه",
            "عدم استفاده از هرگونه اتصال بین لایه‌ها",
            "اینکه نرمال‌سازی همیشه بعد از جمع انجام می‌شود",
            "استفاده از فقط یک زیرلایهٔ توجه بدون شبکهٔ تغذیه‌ای"
          ],
          "answer": 0
        },
        {
          "id": "Q090",
          "question": "چرا محاسبهٔ توجه روی اسناد بسیار طولانی هزینه‌بر است؟",
          "options": [
            "چون اندازهٔ بردارهای توکن‌ها با طول سند افزایش می‌یابد",
            "به‌خاطر محدودیت‌های موازی‌سازی سخت‌افزاری که اجازهٔ پردازش همزمان را نمی‌دهد",
            "زیرا باید ضرب‌های اسکالر بین هر جفت توکن انجام شود که باعث رشد رفتاری هزینه با تعداد جفت‌ها می‌شود",
            "چون ماسک اعمال‌شده برای حذف آینده باعث محاسبات اضافی زیادی می‌شود"
          ],
          "answer": 2
        }
      ]
    },
    {
      "chunk_id": "CHUNK_10",
      "questions": [
        {
          "id": "Q091",
          "question": "ترتیب درست عملیات در یک بلوک ترانسفورمر از ورودی تا خروجی کدام است؟",
          "options": [
            "ابتدا نرمال‌سازی لایه روی ورودی، سپس توجه چندسر، جمع با ورودی، دوباره نرمال‌سازی، شبکه پیش‌خور و در نهایت جمع با حالت میانی",
            "ابتدا توجه چندسر، سپس نرمال‌سازی، بعد شبکه پیش‌خور و در پایان جمع با ورودی",
            "ابتدا شبکه پیش‌خور، سپس نرمال‌سازی، بعد توجه چندسر و در نهایت جمع با حالت میانی",
            "ابتدا نرمال‌سازی، شبکه پیش‌خور، توجه چندسر و در آخر جمع با ورودی"
          ],
          "answer": 0
        },
        {
          "id": "Q092",
          "question": "وقتی گفته می‌شود همان شبکه پیش‌خور به‌طور موازی روی هر یک از بردارهای پنجره اعمال می‌شود، منظور چیست؟",
          "options": [
            "یک نسخه متفاوت از شبکه برای هر موقعیت توکن استفاده می‌شود",
            "همان پارامترهای شبکه به‌صورت جداگانه روی هر بردار موقعیت اعمال می‌شود",
            "شبکه پیش‌خور فقط روی بردار میانگین تمام توکن‌ها اجرا می‌شود",
            "شبکه پیش‌خور تنها روی اولین توکن پنجره اجرا می‌شود"
          ],
          "answer": 1
        },
        {
          "id": "Q093",
          "question": "کدام گزاره درباره نرمال‌سازی درون بلوک ترانسفورمر صحیح است؟",
          "options": [
            "نرمال‌سازی فقط روی بردار خروجی نهایی انجام می‌شود",
            "نرمال‌سازی مقادیر توجه را از طریق تمام توکن‌ها جمع می‌کند",
            "نرمال‌سازی جایگزین جمع باقیمانده می‌شود",
            "هر توکن به‌طور مستقل و موازی در لایه نرمال‌سازی پردازش می‌شود"
          ],
          "answer": 3
        },
        {
          "id": "Q094",
          "question": "چرا ابعاد ورودی و خروجی بلوک‌های ترانسفورمر یکسان نگه داشته می‌شوند؟",
          "options": [
            "برای کاهش تعداد پارامترها در مدل",
            "برای آسان‌تر کردن محاسبات توجه چندسر",
            "تا بتوان بلوک‌ها را به صورت متوالی روی هم قرار داد و اتصال باقی‌مانده برقرار بماند",
            "برای اینکه بردارهای موقعیت و کلمه نیاز به یک اندازه متفاوت نداشته باشند"
          ],
          "answer": 2
        },
        {
          "id": "Q095",
          "question": "چگونه بردار توکنی برای هر کلمه از ماتریس نگاشت استخراج می‌شود؟",
          "options": [
            "با استفاده از شاخص واژه و انتخاب سطر متناظر در ماتریس نگاشت",
            "با میانگین‌گیری از همه سطرهای ماتریس نگاشت",
            "با ضرب ماتریس موقعیت در ماتریس نگاشت",
            "با اعمال لایه توجه روی تمام سطرها و انتخاب اولین خروجی"
          ],
          "answer": 0
        },
        {
          "id": "Q096",
          "question": "اگر یک بردار یک‌تایی (one-hot) را در ماتریس نگاشت ضرب کنیم، چه نتیجه‌ای حاصل می‌شود؟",
          "options": [
            "جمع همه سطرها به‌دست می‌آید",
            "تبدیل ماتریس به بردار احتمال رخ می‌دهد",
            "ستون‌های ماتریس جابه‌جا می‌شوند",
            "سطر مربوط به آن یک در بردار یک‌تایی استخراج می‌شود"
          ],
          "answer": 3
        },
        {
          "id": "Q097",
          "question": "کدام توصیف مربوط به روش ساده‌ی موقعیت‌یابی مطلق است؟",
          "options": [
            "موقعیت‌ها با قواعد دستوری از روی کلمات استخراج می‌شوند",
            "برای هر موقعیت ورودی یک بردار موقعیت یادگرفتنی و جداگانه وجود دارد که همراه بردار کلمه افزوده می‌شود",
            "موقعیت‌ها به صورت دینامیک از توجه چندسر محاسبه می‌شوند و ذخیره نمی‌شوند",
            "بردار موقعیت فقط پس از آموزش کامل مدل محاسبه و اضافه می‌شود"
          ],
          "answer": 1
        },
        {
          "id": "Q098",
          "question": "پس از جمع بردار کلمه و بردار موقعیت چه چیزی به دست می‌آید؟",
          "options": [
            "یک بردار جدید با بعدی متفاوت که برای توجه چندسر لازم است",
            "یک بردار جدید که طول آن دو برابر بردار کلمه است",
            "یک بردار ورودی جدید با همان طول بردار اصلی که برای مراحل بعدی استفاده می‌شود",
            "فقط بردار موقعیت باقی می‌ماند و بردار کلمه نادیده گرفته می‌شود"
          ],
          "answer": 2
        },
        {
          "id": "Q099",
          "question": "نمایش یک توالی به‌صورت ماتریسی از بردارهای یک‌تایی به چه معناست؟",
          "options": [
            "هر سطر ماتریس نشان‌دهنده یک بردار یک‌تایی برای موقعیت متناظر در پنجره است",
            "هر ستون ماتریس نشان‌دهنده یک بردار توکن است",
            "هر خانه ماتریس احتمال وقوع یک واژه در همه موقعیت‌ها را نشان می‌دهد",
            "این ماتریس همان ماتریس بردارهای موقعیت است"
          ],
          "answer": 0
        },
        {
          "id": "Q100",
          "question": "کدام گزاره درباره ماتریس نگاشت واژگان صحیح است؟",
          "options": [
            "این ماتریس یک سطر برای هر نماد در واژگان دارد و هر سطر بردار نگاشت آن نماد است",
            "هر ستون این ماتریس مربوط به یک نماد واژگان است",
            "این ماتریس بردارهای موقعیت را نگهداری می‌کند نه بردارهای کلمه",
            "در این ماتریس وزن‌های لایه توجه ذخیره می‌شود"
          ],
          "answer": 0
        }
      ]
    },
    {
      "chunk_id": "CHUNK_11",
      "questions": [
        {
          "id": "Q101",
          "question": "نمایش نهایی هر توکن در ماتریس ورودی چگونه ساخته می‌شود؟",
          "options": [
            "با جمع بردار جاسازی شناسهٔ توکن و بردار جاسازی موقعیت همان مکان",
            "فقط از بردار جاسازی شناسهٔ توکن استفاده می‌شود",
            "فقط از بردار جاسازی موقعیت استفاده می‌شود",
            "بردار نمایش برای هر توکن به‌صورت تصادفی اختصاص داده می‌شود"
          ],
          "answer": 0
        },
        {
          "id": "Q102",
          "question": "کدام یک از مشکلات روش سادهٔ جاسازی موقعیت در فرایند آموزش مدل اشاره شده است؟",
          "options": [
            "در همهٔ موقعیت‌ها دادهٔ آموزشی به یک اندازه موجود است",
            "این روش باعث افزایش قابل توجه پارامترهای مدل می‌شود",
            "برای موقعیت‌های نزدیک ابتدای ورودی دادهٔ آموزشی فراوان و برای موقعیت‌های دورتر کمتر است که ممکن است منجر به آموزش ضعیف آنها و عدم تعمیم خوب شود",
            "بردارهای موقعیت موجب کاهش طول توالی ورودی می‌شوند"
          ],
          "answer": 2
        },
        {
          "id": "Q103",
          "question": "چه نوع جایگزینی برای مقابله با مسئلهٔ طول‌های مختلف توالی‌ها مطرح شده است؟",
          "options": [
            "استفاده از بردارهای موقعیت یادگرفتنی جدا برای هر شاخص",
            "انتخاب یک تابع ایستا که اعداد صحیح را به بردارهای حقیقی نگاشت می‌دهد، مانند ترکیب سینوس و کسینوس با فرکانس‌های مختلف",
            "اختصاص بردارهای تصادفی ثابت به هر موقعیت",
            "حذف هرگونه اطلاعات موقعیت از ورودی"
          ],
          "answer": 1
        },
        {
          "id": "Q104",
          "question": "جاسازی‌های مبتنی بر توابع سینوسی چه ویژگی مهمی را در رابطه با موقعیت‌ها منعکس می‌کنند؟",
          "options": [
            "آنها معنای معنایی کلمات را بهبود می‌بخشند",
            "آنها ترتیب و نزدیکی بین موقعیت‌ها را بهتر نشان می‌دهند، به‌طوری که موقعیت‌های نزدیک‌تر نمایش‌های مشابه‌تری دارند",
            "آنها صرفاً جایگاه‌های دور را تقویت می‌کنند",
            "آنها محاسبات توجه را حذف می‌کنند"
          ],
          "answer": 1
        },
        {
          "id": "Q105",
          "question": "روش‌های پیشرفته‌تر موقعیت‌یابی چه تفاوتی نسبت به افزودن یک‌بارهٔ بردار موقعیت در ابتدای ورودی دارند؟",
          "options": [
            "به‌جای نمایش مطلق، موقعیت نسبی بین توکن‌ها را نشان می‌دهند و معمولاً در مکانیزم توجه در هر لایه اعمال می‌شوند",
            "همهٔ اطلاعات موقعیت را در اولین مرحلهٔ پردازش حذف می‌کنند",
            "فقط برای مدل‌های کوچک کاربرد دارند",
            "بهتر است قبل از هر لایه حذف شوند"
          ],
          "answer": 0
        },
        {
          "id": "Q106",
          "question": "در متن، منظور از عبارت «language modeling head» یا سر مدل زبانی چیست؟",
          "options": [
            "یک تابع هزینهٔ ویژه برای تنظیم وزن‌ها در آموزش",
            "یک مدار یا ماژول اضافه که بر بالای ساختار پایهٔ ترنسفورمر افزوده می‌شود تا مدل‌سازی زبان را انجام دهد",
            "یک نوع لایهٔ توجه اختصاصی برای ترجمه",
            "روشی برای فشرده‌سازی بردارهای خروجی"
          ],
          "answer": 1
        },
        {
          "id": "Q107",
          "question": "وظیفهٔ اصلی مدل‌های زبانی چگونه توصیف شده است؟",
          "options": [
            "فشرده‌سازی متن به بردارهای کوچک برای ذخیره‌سازی",
            "تعیین زبان نوشته‌شده در یک متن",
            "با در نظر گرفتن زمینهٔ قبلی، تخصیص احتمالات مشروط به هر واژهٔ ممکن برای پیش‌بینی واژهٔ بعدی و تولید یک توزیع روی واژگان",
            "تبدیل متن به نمایش‌های تصویری"
          ],
          "answer": 2
        },
        {
          "id": "Q108",
          "question": "در مدل‌های ترنسفورمر، منظور از اندازهٔ بافت (context) چیست؟",
          "options": [
            "تعداد توکن‌هایی است که مدل می‌تواند به‌عنوان زمینه برای پیش‌بینی در نظر بگیرد و این مقدار می‌تواند برای مدل‌های بزرگ بسیار زیاد باشد، مثلاً ده‌ها هزار توکن",
            "تعداد نمونه‌های آموزشی موجود در مجموعهٔ داده",
            "تعداد پارامترهای قابل آموزش در مدل",
            "تعداد لایه‌های شبکه"
          ],
          "answer": 0
        },
        {
          "id": "Q109",
          "question": "سر مدل زبانی چگونه از خروجی لایهٔ آخر برای پیش‌بینی کلمهٔ بعدی استفاده می‌کند؟",
          "options": [
            "خروجی لایهٔ آخر را مستقیماً به متن تبدیل می‌کند بدون هیچ نگاشتی",
            "فقط بردار اولین توکن را برای پیش‌بینی به‌کار می‌گیرد",
            "تمام بردارهای لایهٔ آخر را با هم جمع کرده و سپس حذف می‌کند",
            "بردار متناظر با آخرین توکن در خروجی را می‌گیرد و با استفاده از آن توزیع احتمال روی واژگان برای پیش‌بینی کلمهٔ بعدی تولید می‌کند"
          ],
          "answer": 3
        },
        {
          "id": "Q110",
          "question": "نقش اولین ماژول در فرایند تبدیل خروجی آخرین توکن به توزیع احتمالات واژگانی چیست؟",
          "options": [
            "اعمال یک فیلتر حذف نویز روی بردارها",
            "یک لایهٔ خطی که خروجی بردار آخرین توکن را به صورت نگاشتی مناسب برای تولید احتمالات واژگان تبدیل می‌کند",
            "اعمال یک تابع فعال‌سازی غیرخطی پیچیده برای ساختاردهی بردارها",
            "بازآرایی تصادفی ترتیب بردارهای خروجی"
          ],
          "answer": 1
        }
      ]
    }
  ]
}